# Proxmox VE NAS and Gaming VMs - 03 - Proxmox Tweaks

Various changes I make as soon as installation is done. 

## Table of Contents:
> **Hint:** Github has a drop-down automatic TOC, find the **â‰¡** icon on the top-left

* ^ [README](README.md)  (*links to* ***other pages***)
* 03 - Proxmox Tweaks (***this page***)
    + [03.A. System Setup](#03a-system-setup)
        - [03.A.i. Free/non-Subscription Repo](#03ai-freenon-subscription-repo)
        - [03.A.ii. System Update](#03aii-system-update)
    + [03.B. UI Adjustments](#03b-ui-adjustments)
    + [03.C. Minimizing SSD Wear](#03c-minimizing-ssd-wear)
        - [03.C.i. Disable Proxmox High Availability Services](#03ci-disable-proxmox-high-availability-services)
        - [03.C.ii. log2ram - Move Frequently Written Files to RAM](#03cii-log2ram---move-frequently-written-files-to-ram)
            * [03.C.ii.a. install](#03ciia-initial-install)
            * [03.C.ii.b. moving other stuff to RAM](#03ciib-moving-other-stuff-to-ram)
        - [03.C.iii. Setting swappiness](#03ciii-setting-swappiness)
    + [03.D. Lowering ZFS RAM use](#03d-lowering-zfs-ram-use)
	- [03.E Results](#03e-results)
    + [03.F. Fix Missing Drivers](#03f-fix-missing-drivers)
        - [03.F.i. 'regulatory.db'](#03fi-regulatorydb)
        - [03.F.ii. FAILED fixes for 'iwlwifi' and 'thermal_zone2'](#03fii-failed-fixes-for-iwlwifi-and-thermal_zone2)
* \> [04. Proxmox Extras](04.ProxmoxExtras.md) (*next page*)

---

## 03.A. System Setup

***Recommended*** changes.

### 03.A.i Free/non-Subscription Repo

* Navigate to 'Datacenter' > {your Proxmox host name} > 'Updates' > 'Repositories'
    + **No need to do this if you DO have a subscription. But for the home user this is needed to get updates running.** 
    + click the 'Add' button and add the "No-Subscription" repo
    + click the line for "https://enterprise.proxmox.com/debian/pve" (components has "pve-enterprise") and then click the 'Disable' button

---

### 03.A.ii System Update

* 'Datacenter' > {your machine} > 'Updates'
    + 'Refresh' (and click the top-right 'X' when you see "TASK OK")
    + '>_ Upgrade'
        - Opens a separate root shell window
        - If output looks good, hit {Enter} to accept the 'Y/n' prompt and let it run until done
            * It is done when shell prompt returns
            * look to see if the last message recommends a reboot
            * close the shell window
        - If reboot recommended, press 'Reboot' button in the UI
        - If this update included a kernel update
            * You will see multiple kernels to select on boot
            * Default is going to be the kernel you just installed

---

## 03.B. UI Adjustments

*This section is* ***VERY Optional*** *and modifies system files in a way that can be overridden in Proxmox updates.* These are unofficial changes to the UI. 

**NOTES:**

* These just tune the UI to be friendlier
* You **will** need to re-apply these after *some* Proxmox updates (I had to do after first 7.1 system update, which is why I do that first)
* You can revert these easily

...

**Dark Mode from Weilbyte**

*NOTE: Not really needed anymore, Proxmox added a Dark Mode that can be set to default. Yay.*

+ [Install from the PVEDiscordDark github repo](https://github.com/Weilbyte/PVEDiscordDark) 
	+ Use the 'Oneliner' command
	+ Use the instructions there then return here

**Remove "No Subscription" popups from Dannyda's script**

*NOTE: The [method shown here](https://dannyda.com/2020/05/17/how-to-remove-you-do-not-have-a-valid-subscription-for-this-server-from-proxmox-virtual-environment-6-1-2-proxmox-ve-6-1-2-pve-6-1-2/) no longer worked for me on PVE 8.1.3, but may have been user error. I found it better for me to just do it manually:*

- First make a copy of the original file:
`cp /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js.orig`

- Edit file:
`nano /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js`
  
- [control]+w to search for "no valid", section should currently look similar to:

 
>                     if (res === null || res === undefined || !res || res
>                        .data.status.toLowerCase() !== 'active') {
>                        Ext.Msg.show({
>                            title: gettext('No valid subscription'),
>                            icon: Ext.Msg.WARNING,
>                            message: Proxmox.Utils.getNoSubKeyHtml(res.data.url),
>                            buttons: Ext.Msg.OK,
>                            callback: function(btn) {
>                                if (btn !== 'ok') {
>                                    return;
>                                }
>                                orig_cmd();
>                            },
>                        });
>                    } else {
>                        orig_cmd();
>                    }


- Save file

- `systemctl restart pveproxy.service`

- Reload the Proxmox UI and test by doing [Host] > Updates > Refresh

* To revert to the original:  
	`cp /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js.orig /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js && systemctl restart pveproxy.service`
+ ***Alternative:*** [PVE Nag Buster](https://github.com/foundObjects/pve-nag-buster/) is an apt installed package that will keep track of updates to 'proxmoxlib.js' and patch them each time they revert to original from a Proxmox update. 
	- found here [Reddit thread](https://www.reddit.com/r/Proxmox/comments/tgojp1/removing_proxmox_subscription_notice/)
	- *I don't use this to keep from adding another repository that will auto-update a non-Proxmox/Debian package.* Your choice may vary. **I did not test it.** 

---

## 03.C. Minimizing SSD Wear

*I recommend doing this if running on consumer hardware.* If using multiple Proxmox systems and/or Enterprise class storage, you probably don't want some of this. I've put extra details on each step for you to decide. 

Proxmox is built with Enterprise-class disks in mind. It is still fine for a consumer-class PC in a home environment, just not built with that in mind. Proxmox does **a lot** of writes on the root disk by default. If your boot drive is a consumer SSD it can wear out very quickly (months from 0% to 100% wear) without some tweaks. 

Consumer SSDs vary widely in the type of flash they use, from of SLC (rare these days), MLC, TLC, QLC, PLC, etc. SLC writes 1 bit per cell, MLC does 2, TLC 3, QLC 4, PLC 5. This increases storage size per cell (good) but each increase lowers the overall lifetime of that cell (bad). [More information](https://www.howtogeek.com/444787/multi-layer-ssds-what-are-slc-mlc-tlc-qlc-and-mlc/). 

TLDR; the cheaper the drive is per TB = less liftime write endurance. And Proxmox can write a lot of data even when idle. 

* **Get an idea of how much is written every few seconds without tuning:**
    + `zpool iostat 2`
		- This runs the command every 2 seconds
        - The command adds more writes when running, so total below is bit higher than true idle, but will still show a massive difference after tuning
        - changing the interval affects how much extra write happens(change "2" to "1" to see write total double, change to "5" to see write total less than half of "2", etc)
	- On my system at this point I see 250-500K writes every 6 seconds

### 03.C.i. Disable Proxmox High Availability Services

*This is* ***only for a single-node Proxmox install*** (ie, you're **not clustering / using high availability**). You can enable these services later on if you decide to use clustering (not personally tested). These services do a lot of constant writes. 

I hope future versions of Proxmox give UI options to cache more data. Until then, since this config doesn't need these for single-node, turn them off.

If you do plan to use HA / Clustering, don't change these. The 'log2ram' section below can still help. 

```bash
# Copy/Paste this block into a root bash shell
systemctl disable pve-ha-lrm
systemctl disable pve-ha-crm
systemctl disable corosync.service
systemctl disable pvesr.timer
```

**NOTE:** `systemctl disable pvesr.timer` gave an error. I had never configured Replication, so don't worry about the message. [Info about 'pvesr'](https://pve.proxmox.com/pve-docs/pvesr.1.html). 

### 03.C.ii. log2ram - Move Frequently Written Files to RAM

[log2ram](https://github.com/azlux/log2ram) creates a tmpfs ramdisk and mounts it at '/var/log'. It will perform a copy daily of the data to a specified drive. If the system is gracefully shudown it will also copy the log to disk.

* Pro: saves the constant writing to log files during normal operation
* Con: **an ungraceful shutdown/reboot, including a kernel panic, won't flush the log to disk*

Before installing, decide which is more important to you, *fewer writes* or a more *default Proxmox install*.

* [More about 'log2ram'](https://linuxfun.org/en/2021/01/01/what-log2ram-does-en/)
* [Proxmox Forums about 'log2ram'](https://forum.proxmox.com/threads/proxmox-usb-bootstick-mit-log2ram-oder-folder2ram.76583/) 
	* in German, Google Translate does a good job
	* Mentions possibly using 'folder2ram' instead of 'log2ram'
	* After looking at the code and install needs for both I stayed with 'log2ram'
	* 'folder2ram' gives an option that doesn't need a 3rd party apt repo

#### 03.C.ii.a. Install

* Adds a new apt repository, 'http://packages.azlux.fr/debian/'
* Be aware of this if you have security concerns with 3rd party packages

```bash
# Copy/Paste this block into a root bash shell
echo "deb [signed-by=/usr/share/keyrings/azlux-archive-keyring.gpg] http://packages.azlux.fr/debian/ bullseye main" | tee /etc/apt/sources.list.d/azlux.list
wget -O /usr/share/keyrings/azlux-archive-keyring.gpg  https://azlux.fr/repo.gpg
apt update
apt install log2ram
```

**Reboot?**

* *If you only want logs in RAM*, ***not anything like HA services or UI graphs***, reboot now
* Otherwise, don't reboot yet

#### 03.C.ii.b. moving other stuff to RAM

*Optional:* Folders I move to RAM (in addition to default '/var/log') using 'log2ram':

* '/var/lib/pve-cluster' ... Proxmox clustering info, frequently written (if services running)
* '/var/lib/rrdcached' ... Used this for UI graphs like "Summary" (possibly other places), frequently written
    + **NOTE:** after moving '/var/lib/rrdcached' to RAM I noticed gaps in graphs after rebooting.
        - This gap can be long if doing multiple reboots, 15+ minutes of missing graphs
        - I assume syncing rrdcached to disk doesn't work as fully with log2ram, I doubt this would change with folder2ram
        - Not critical, graphs update once system boots (wait a minute or two) but if this bugs you, don't move rrdcached to RAM. 
    + *An alternative to moving rrdcached to RAM* while lowering writes is to increase the time for [writes / flushs](https://forum.proxmox.com/threads/reducing-rrdcached-writes.64473/) but I haven't yet tried this. [More info](https://www.systutorials.com/docs/linux/man/1-rrdcached/)

Folders I do NOT to move to RAM, for now, *that might save more writes*:

* '/var/tmp' ... at least some chance that this messes with shared storage (second page of the linked forum post)
* '/var/cache' ... would likely be fine but it has the chance to get very large

*Configuring 'log2ram' is very easy.*

* **NOTES:**
	* The README for 'log2ram' mentions a config option for 'RSYNC'
		* This is an older version option
		* Current version of log2ram will use rsync as default if it is available (t is for Proxmox VE 7.1)
		* Filed an [issue for this](https://github.com/azlux/log2ram/issues/188) to update the README.
	* The config mentions ZRAM / ZL2R
		* Often used on raspberry pi systems
		* I haven't used it. 
    + Read comments in the file for more information
* Edit **'/etc/log2ram.conf':**
    + Change  
      'SIZE=40M'  
      to  
      'SIZE=400M'
        - 10x the original size is still only 0.3% of 128GB, to minimize risk of overflow
        - Put what you feel comfortable with on *your* system
    + Change  
      'PATH_DISK="/var/log"'  
      to  
      'PATH_DISK="/var/log;/var/lib/pve-cluster;/var/lib/rrdcached"'
    + Change  
      'LOG_DISK_SIZE=100M'  
      to  
      'LOG_DISK_SIZE=1000M'
        - Done to match the increase in RAM use. 
        - I'm not sure this needed to be done, have filed a [question against it](https://github.com/azlux/log2ram/issues/188), will update depending on answer

*What will change?*

* Directories will be renamed:
    + '/var/log' > '/var/hdd.log' (default)
    + '/var/lib/pve-cluster' > '/var/lib/hdd.pve-cluster' *(IF done below)*
    + '/var/lib/rrdcached' > '/var/lib/hdd.rrdcached' *(IF done below)*
* "New" Directories mounted on tmpfs:
    + '/var/log' (default)
    + '/var/lib/pve-cluster' *(IF done below)*
    + '/var/lib/rrdcached' *(IF done below)*
* On boot log2ram will rsync from the renamed 'hdd.\*' versions into RAM
* On sync (automatically on shutdown/reboot, once per day otherwise) log2ram will rsync the tmpfs mounts to the 'hdd.\*' directories

### 03.C.iii. Setting swappiness

* "swappiness" tells the kernel how much to swap RAM to disk
	* Value is essentially a percentage
    + Default in Proxmox VE 7.1 is 60 (start swapping when RAM is reaching 60% use)
        - See the current setting via  
          `sysctl vm.swappiness`  
          or  
          `cat /proc/sys/vm/swappiness`
    + See the current swap **usage** (likely 0 at this point) via `free`

**Important:** 'swappiness' changed in Linux kernal 3.5 (backported to 2.6.something). If searching for info be aware of old info where "swappiness=0" behaved differently. Make sure info you use is up to date. 

***WARNING:*** **There are many considerations about changing this.** I'm not going decide for you, only show what I've used. ***Read stuff before you do any changes.***

***Reminder:*** Ubuntu is based on Debian, as is Proxmox, don't worry about the distribution name in the links below:

* [Wikipedia](https://en.wikipedia.org/wiki/Memory_paging#Swappiness) ... intro level information
* [Ubuntu beginner info](https://help.ubuntu.com/community/SwapFaq#What_is_swappiness_and_how_do_I_change_it.3F) ... easy to read and you might want to read the whole page about swap
* [more Ubuntu info](https://dev.to/stackallflow/how-to-configure-swappiness-in-ubuntu-jkp)
* [Debian/Linux notes](https://www.mybluelinux.com/how-disable-swap-in-debian-or-linux-system/)
* [What is Swappiness On Linux?](https://www.howtogeek.com/449691/what-is-swapiness-on-linux-and-how-to-change-it/)

*I decided to try swappiness=10* and tune as needed. Swap won't be engaged until 80-90% of RAM used. 1, 5 or 20 are other options. I've seen [mentions](https://forum.proxmox.com/threads/swappiness-to-0-doesnt-seems-to-work.38439/) using 0 (disable swapping) but I'm not comfortable with that. *Or just leave it at default 60.* 

* Edit '/etc/sysctl.conf' to add "vm.swappiness = 10" to the bottom

```bash
# Copy/Paste this block into a root bash shell
FILE=/etc/sysctl.conf
cat << EOF >> $FILE

# ADDED: vm.swappiness to lower swap use, default (no value present) = 60
vm.swappiness = 10
EOF
```

**NOTES:** 

* I seem to remember seeing a comment that Proxmox can override this file with a default version. Check after system updates.
* [4 year old issue](https://forum.proxmox.com/threads/swappiness-question.42295/) with Proxmox that I need to verify is fixed (once I have containers running). If it is, also look into [this](https://forum.proxmox.com/threads/swap-using-in-proxmox-6-2.72888/). (also: this is [Related](https://github.com/fulgerul/ceph_proxmox_scripts/blob/master/swapoff.sh))


---

### 03.D. Lowering ZFS RAM use

***Optional.*** *If not using ZFS, completely skip this.*

**NOTE:** My system has 128GB of RAM but multiple VMs and containers. If you have less RAM or want to emphasize ZFS performance, leave defaults and allow ZFS ARC to free memory as needed. 

By default ZFS on Proxmox uses up to 50% of RAM for ARC (caching). This change lowers the amount of RAM ZFS is able to consume (64GB is overkill deduplicating large amounts, which I'm not doing). 

*The reason this is optional:* if you don't change this and system requests RAM for another application ZFS will release ARC RAM. But as I have some large VMs that I know will be frequently running, I prefer to keep that RAM open at all times. 

[Good reading](https://www.cyberciti.biz/faq/how-to-set-up-zfs-arc-size-on-ubuntu-debian-linux/)

* To see ZFS ARC min/max is *currently*:  
  `cat /proc/spl/kstat/zfs/arcstats | grep "^c_[min|max]"`
    + c_min is the minimum ARC size in bytes
    + c_max is the maximum ARC size in bytes
* on a system with 128GB of RAM the output looks like:

<pre>c_min                           4    4218658688
c_max                           4    67498539008</pre>

On a freshly installed server without anything extra running you won't see full ARC RAM use. If you want to see it, set up a long file transfer (I copied 8TB from an old drive to my 'xfer' pool showing 64GB/50% RAM use for hours). *Optional.*

See [this](https://www.solaris-cookbook.eu/solaris/solaris-10-zfs-evil-tuning-guide/) for Solaris ZFS background info on tuning ZFS. Don't use commands there as they can be different for Solaris ZFS (vs our Linux + openZFS), but it is good information if you want to know more background on tuning ZFS. 

* ZFS specs for Solaris recommend 2GB ARC minimum
* From reading various Proxmox/Debian threads, I want minimum 4GB ARC (which is the default already).
* Ideas ([from here](https://forums.servethehome.com/index.php?threads/how-much-ram-is-required-for-zfs.13463/)) for maximum size:
    + Add enough to accomodate network traffic ... 500MB ARC for each 1Gbps of bandwidth (ie, 10Gbps = 5GB)
        - currently running 2Gbps max network (2 x interfaces connected to 1Gb switch)
        - Containers and machines using VirtIO can act like a network device with much higher throughput ... 10Gbps seems a good ballpark for VirtIO, so ... 5GB ARC
    + 1GB for every TB of disk in the pool to fully cache metadata
        - Currently have 18GB of usable storage. So ... 18GB
        - But I'm going to up that over time so I'll assume 32GB
    + Add those up for your system and add however much more you can afford (small reads from multiple users or containers)
    + If you go over 50%, just leave things default, no edits below
    + I feel 37GB is higher than I actually need so I'm giving 32GB
        - *Reminder:* ZFS will free RAM from ARC as needed so a higher value isn't going to break things
    + **IMPORTANT**: this does NOT take into account if you are going to use [ZFS deduplication](https://www.oracle.com/technical-resources/articles/it-infrastructure/admin-o11-113-size-zfs-dedup.html).
        - Add 2-3GB of RAM used for each 1TB of deduplicated storage ... I'm not using dedupe so I'm not researching an exact amount
        - If you are going to use dedupe ... consider setting it **only on a ZFS dataset**, not your entire pool, to minimize RAM required

* *Paste this and run it* (reminder: as root in bash)
    + If you have more or less RAM, tweak the byte values to what you want using info above as a guide
    + In my case the minimum I set here was the same as the default (4GB)
	    + I've seen notes that 'zfs_arc_max' requires 'zfs_arc_min' to load
	    + If the min is set too low or the same as the default, ZFS will ignore 'zfs_arc_min' but still set 'zfs_arc_max'

```bash
# Copy/Paste this block into a root bash shell
FILE=/etc/modprobe.d/zfs.conf
if test -f "$FILE"; then
    echo; echo "WARNING: $FILE exists. Edit it manually."; echo
else
    echo; echo "Creating new file: $FILE"; echo
    cat << EOF > $FILE
# ZFS ARC size
# minimum 4GiB (in bytes)
options zfs zfs_arc_min=4294967296
# maximum 32GB (in bytes)
options zfs zfs_arc_max=34359738368
EOF
    echo; echo "Done. $FILE contents:"; echo
    cat $FILE; echo
fi
```

* **Update initramfs (only choose **one** of these options). 
    + Update ONLY the kernel currently running:  
      `update-initramfs -u`  
      *Or* ***update ALL installed kernels*** *(this is what I use)*:  
       `update-initramfs -u -k all`  
       ('proxmox-boot-tool refresh' won't update everything, use the above)
* *Reboot to apply the change*
* Login and see what your ZFS ARC is *currently* able to use  
  `cat /proc/spl/kstat/zfs/arcstats | grep "^c_[min|max]"`
     - c_min is the minimum ARC size in bytes
     - c_max is the maximum ARC size in bytes
    + After applying changes I see:

<pre>c_min                           4    4294967296
c_max                           4    34359738368</pre>

---

### 03.E. Results

* **Reboot** if you haven't already (ie, you didn't tune ZFS ARC)
* Verify the swappiness, if changed, via  
  `sysctl vm.swappiness`
* With all changes above:  
  `zpool iostat 2` 
  shows 400-500K every minute instead of every 4-5sec
   + This can probably be lowered more but I'm OK with this
   + Rough estimates on this SSD with 150TBW in the warranty (which ends after 5 years) would be 150+ years of use if nothing else were doing writes
    + Installing `fatrace` gives some information (`apt install fatrace`) but I can't see the 500K writes at all, only small writes every few seconds to '/var/lib/pve-manager' and '/var/lib/chrony'. These might be candidates for log2ram. 
* [Note](https://www.reddit.com/r/Proxmox/comments/ncg2xo/minimizing_ssd_wear_through_pve_configuration/) about monitoring writes on a daily basis, search for smartctl, if you want to keep an eye on this going forward (I haven't done this yet)

---

## 03.E. Fix Missing Drivers

*Optional*

Nothing here is critical, feel free to skip to the next page. Including as info on what you might do to fix a driver issue. 

On Debian Bullseye's initial release I had a few drivers that required the 'non-free' firmware. With Proxmox VE 7.1 I didn't have much to fix. So this is a short list and only the first is actually fixed.

With `dmesg` I see 3 items to try to fix ("{snip}" means I cut out other lines):

<pre>
{snip}
[   13.803059] cfg80211: failed to load regulatory.db
{snip}
[   13.875426] Bluetooth: hci0: Failed to load Intel firmware file intel/ibt-20-1-3.sfi (-2)
{snip}
[   13.922657] thermal thermal_zone2: failed to read out thermal zone (-61)
{snip}
</pre>


### 03.F.i. 'regulatory.db'

*Error being fixed:*

> [   13.803059] cfg80211: failed to load regulatory.db

This related to wifi drivers and apparently a long-standing issue [as per this](https://kernel.googlesource.com/pub/scm/linux/kernel/git/sforshee/wireless-regdb/+/refs/heads/master). I'm not actually using the WIFI adapter so this could just be skipped, but it reminds me how to do it later if needed. 

```bash
# Copy/Paste this block into a root bash shell
mkdir tempdownload
cd tempdownload
wget https://kernel.googlesource.com/pub/scm/linux/kernel/git/sforshee/wireless-regdb/+archive/refs/heads/master.tar.gz
tar zxvf master.tar.gz
mv regulatory.db /lib/firmware
mv regulatory.db.p7s /lib/firmware
cd ..
rm -rf tempdownload
echo; echo "Done.";echo
```

Fixed. After rebooting (*no need to do reboot now unless you really want to see this now*), `dmesg | grep regulatory` shows this:

> [   13.680561] cfg80211: Loading compiled-in X.509 certificates for regulatory database

### 03.F.ii. FAILED fixes for 'iwlwifi' and 'thermal_zone2'

**WARNING: This is** ***NOT WORKING*** **as of this writing.** 

2 errors that, after investigating, I'm not going to try and fix. I'm leaving this info in case I look at it in the future. I don't  have a need for Bluetooth on the host (might pass through to a VM later).

**NOTE:** again, the following didn't fix anything, it is just a log of what was looked at. **Don't run any commands here.**

*Error being investigated:*

[   13.875426] Bluetooth: hci0: Failed to load Intel firmware file intel/ibt-20-1-3.sfi (-2)

Part of the "non-free" drivers. If not using Bluetooth on the host, ignore. 

**WARNING:** To get the package on a stock Debian system, modify '/etc/apt/sources.list' and add "non-free" to the Debian repos. *Doing that on Proxmox was almost a really bad mistake:* `apt update && apt upgrade` gave this after adding "non-free":

<pre>
{snip}
The following packages will be REMOVED:
 proxmox-ve pve-firmware pve-kernel-5.13
The following NEW packages will be installed:
 firmware-iwlwifi
{snip}
</pre>

**don't do that!**

Next try: install [this package](http://ftp.debian.org/debian/pool/non-free/f/firmware-nonfree/firmware-iwlwifi_20210818-1_all.deb) with `dpkg -i` (which would not get automatic updates) ... but:

<pre>
dpkg: regarding firmware-iwlwifi_20210818-1_all.deb containing firmware-iwlwifi:
pve-firmware conflicts with firmware-iwlwifi
 firmware-iwlwifi (version 20210818-1) is to be installed.

dpkg: error processing archive firmware-iwlwifi_20210818-1_all.deb (--install):
conflicting packages - not installing firmware-iwlwifi
Errors were encountered while processing:
firmware-iwlwifi_20210818-1_all.deb
</pre>

So, **same problem**.

***DEFINITELY DO NOT DO THE FOLLOWING:***

[This Debian bug report](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=975726) (for a different iwlwifi different device) looks like Debian has updated the driver but Proxmox hasn't. 

**WARNING:** This file won't auto-update by system updates (unless added to a future version of the Proxmox firmware package). 

`wget https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/intel/ibt-20-1-3.ddc -O /usr/lib/firmware/intel/ibt-20-1-3.ddc`  
`wget https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/intel/ibt-20-1-3.sfi -O /usr/lib/firmware/intel/ibt-20-1-3.sfi`

***After rebooting an infinite loop on the Virtual Terminal video with Bluetooth disconnect/failed/reset errors and fails to boot.***

*Not pursuing.*

...

*Error being investigated:*

> [   13.922657] thermal thermal_zone2: failed to read out thermal zone (-61)

Asked about this here: [Proxmox forums](https://forum.proxmox.com/threads/trying-to-fix-thermal-thermal_zone2-failed-to-read-out-thermal-zone-61.108064/).

`grep . /sys/class/thermal/thermal_zone2/type` reports: 

> iwlwifi_1

Appears related to the same driver that caused Bluetooth to not load. *Since this is not an important item for me, I'm not going to worry about addressing it.* Just wanted to be sure it wasn't a motherboard sensor. 

---
> [^ [TOP OF PAGE](#proxmox-ve-nas-and-gaming-vms---03---proxmox-tweaks)] ... ***End:*** *Proxmox VE NAS and Gaming VMs - Proxmox Tweaks*
> 
> \> NEXT: [04 - Proxmox Extras](04.ProxmoxExtras.md)
>
> \< PREV: [02 - Proxmox Install](02.ProxmoxInstall.md)
