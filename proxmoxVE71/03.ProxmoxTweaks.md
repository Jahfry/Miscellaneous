# Proxmox VE 7.1 NAS and Gaming VMs - 03 - Proxmox Tweaks

Various changes I make as soon as installation is done. 

## Table of Contents:
> **Hint:** Github has a drop-down automatic TOC, find the **â‰¡** icon on the top-left

* ^ [README](README.md)  (*links to* ***other pages***)
* 03 - Proxmox Tweaks (***this page***)
    + [03.A. UI Adjustments](#03a-ui-adjustments)
    + [03.B. System Setup](#03b-system-setup)
        - [03.B.i Free/non-Subscription Repo](#03bi-freenon-subscription-repo)
        - [03.B.ii System Update](#03bii-system-update)
    + [03.C. Minimizing SSD Wear](#03c-minimizing-ssd-wear)
        - [03.C.i. Disable Proxmox High Availability Services](#03ci-disable-proxmox-high-availability-services)
        - [03.C.ii. log2ram - Move Frequently Written Files to RAM](#03cii-log2ram---move-frequently-written-files-to-ram)
            * [03.C.ii.a. initial install](#03ciia-initial-install)
            * [03.C.ii.b. moving other stuff to RAM](#03ciib-moving-other-stuff-to-ram)
        - [03.C.iii. Setting swappiness](#03ciii-setting-swappiness)
        - [03.C.iv. Results](#03civ-results)
    + [03.D. Lowering ZFS RAM use](#03d-lowering-zfs-ram-use)
    + [03.E. Fix Missing Drivers](#03e-fix-missing-drivers)
        - [03.E.i. 'regulatory.db'](#03ei-regulatorydb)
        - [03.E.ii. FAILED fixes for 'iwlwifi' and 'thermal_zone2'](#03eii-failed-fixes-for-iwlwifi-and-thermal_zone2)
* \> [04. Proxmox Extras](04.ProxmoxExtras.md)
---

## 03.A. UI Adjustments

**NOTES:**

* This section **VERY Optional** and modify system files in a way that can be overridden in Proxmox updates. 
    + May need to re-apply after updates (I had to do just that after my first system update).
    + You can revert these prior to updating to be 100%, but doesn't seem needed.
* I do these first thing as it makes doing the rest of the admin tasks nicer. 

* *Dark Mode*
    + There isn't an official way to get Dark Mode on your Proxmox UI
    + [PVEDiscordDark](https://github.com/Weilbyte/PVEDiscordDark) works very well and can be easily uninstalled if you don't like it. Use the instructions there, including a shift+reload in your browser, and come back here. 

* *Remove the "No Subscription" popups*

    + [Dannyda's script](https://dannyda.com/2020/05/17/how-to-remove-you-do-not-have-a-valid-subscription-for-this-server-from-proxmox-virtual-environment-6-1-2-proxmox-ve-6-1-2-pve-6-1-2/)
        - Keep a copy of the original file: `cp /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js.backup /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js.orig`
            * While Dannyda's script does backup the file (to proxmoxlib.js.backup), if you apply the change twice for whatever reason before Proxmox has replaced it with the original version, the backup will just be the modified file. 
            * Revert to original via: `mv /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js.orig /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js && systemctl restart pveproxy.service`
        - Multiple versions on that page, use the last one (6.4-6 through 7.1-12 and up), just paste the line into a shell
        - shift+reload your browser to load the change on any changes to see them take effect
    + ~~[John's Computer Services](https://johnscs.com/remove-proxmox51-subscription-notice/).~~
        - don't use for now as it blocks some UI elements. I've reported the problem.
    + *Alternative:* [PVE Nag Buster](https://github.com/foundObjects/pve-nag-buster/) is a package that will keep track of updates to 'proxmoxlib.js' and patch them each time they revert to original from a Proxmox update. 
        - found here [Reddit thread](https://www.reddit.com/r/Proxmox/comments/tgojp1/removing_proxmox_subscription_notice/)
        - *I chose not to use this to keep from adding another repository that will auto-update a non-Proxmox/Debian package.* 
        - Your choice may vary. I prefer manually doing this. I did not test it. 

---

## 03.B. System Setup

***Recommended*** changes.

### 03.B.i Free/non-Subscription Repo

* Navigate to 'Datacenter' > [your Proxmox host name] > 'Updates' > 'Repositories'
    + **No need to do this if you DO have a subscription. But for the home user this is needed to get updates running.** 
    + click the 'Add' button and add the "No-Subscription" repo
    + click the line for "https://enterprise.proxmox.com/debian/pve" (components has "pve-enterprise") and then click the 'Disable' button

---

### 03.B.ii System Update

* 'Datacenter' > [your machine] > 'Updates'
    + 'Refresh' (and click the top-right 'X' when you see "TASK OK")
    + '>_ Upgrade'
        - opens a window with a shell
        - if all looks good, hit [enter] to accept the 'Y/n' prompt
        - let it run
            * it's done when you see the shell prompt return
            * look to see if the last message recommends a reboot
            * close the shell window
        - if it recommended reboot, do so (shutdown VMs/etc if running) with the 'Reboot' button in the UI
        - if this included a kernel update
            * you will see multiple kernels to select on boot
            * default is going to be the kernel you just installed

---

## 03.C. Minimizing SSD Wear

Proxmox is made for enterprise-class hardware. It is fine for a consumer-class PC in a home environment, just not built with that in mind. Proxmox puts a lot of use on the root drive by default. If your boot drive is a consumer SSD it can wear out very quickly (months from 0% to 100% wear) without some tweaks. 

Consumer SSDs vary widely in the type of flash they use, from of SLC (rare these days), MLC, TLC, QLC, PLC, etc. SLC writes 1 bit per cell, MLC does 2, TLC 3, QLC 4, PLC 5. This increases storage size per cell (good) but each increase lowers the overall lifetime of that cell (bad). [More information on this](https://www.howtogeek.com/444787/multi-layer-ssds-what-are-slc-mlc-tlc-qlc-and-mlc/). 

TLDR; the cheaper the drive is per TB, the less it can endure writing data. Proxmox can write a lot of data. 

### 03.C.i. Disable Proxmox High Availability Services

*This is* ***only for a single-node Proxmox install*** (ie, you're **not clustering**). You should be able to enable these services later on if you decide to use clustering (not personally tested). These services do a lot of constant writes. 

I hope future versions of Proxmox give UI options to cache more data. Since we don't *need* these for single-node, turn them off.

If you do plan to use HA / Clustering, don't change these. The 'log2ram' section below may help you in that case. 

```bash
# Copy/Paste this block into a root bash shell
systemctl disable pve-ha-lrm
systemctl disable pve-ha-crm
systemctl disable corosync.service
systemctl disable pvesr.timer
```

**NOTE:** `systemctl disable pvesr.timer` gave an error. I had never configured Replication, so don't worry about the message. [Info about 'pvesr'](https://pve.proxmox.com/pve-docs/pvesr.1.html). 

### 03.C.ii. log2ram - Move Frequently Written Files to RAM

[log2ram](https://github.com/azlux/log2ram) creates a ramdisk and mounts it at '/var/log', then will perform a copy daily of the data to a specified drive. If the system is gracefully shudown it will also copy the log to disk.

* Pro: saves the constant writing to log files during normal operation
* Con: **an ungraceful shutdown, including a kernel panic, won't flush the log to disk*

Decide which is more important to you. You can read more about it [here](https://linuxfun.org/en/2021/01/01/what-log2ram-does-en/) along with [this](https://forum.proxmox.com/threads/proxmox-usb-bootstick-mit-log2ram-oder-folder2ram.76583/) (in German but Google Translate does a good job). That thread mentions possibly switching to 'folder2ram' instead of 'log2ram'. More on that later but I stuck with 'log2ram'. 


**NOTES:** 

* The README for log2ram mentions a config option for 'RSYNC', this is an older version option. Current version of log2ram will use rsync as default if it is available, which it is by default on Proxmox VE. I've filed an [issue for this](https://github.com/azlux/log2ram/issues/188).
* The config talks about ZRAM / ZL2R ... this appears to be for raspberry pi systems and I haven't used it. 

#### 03.C.ii.a. initial install

```bash
# Copy/Paste this block into a root bash shell
echo "deb [signed-by=/usr/share/keyrings/azlux-archive-keyring.gpg] http://packages.azlux.fr/debian/ bullseye main" | tee /etc/apt/sources.list.d/azlux.list
wget -O /usr/share/keyrings/azlux-archive-keyring.gpg  https://azlux.fr/repo.gpg
apt update
apt -y install log2ram
```

* Reboot now *if you only want logs in RAM*, ***not anything that is actively used by the PVE UI or HA services***
* Otherwise don't reboot yet.

#### 03.C.ii.b. moving other stuff to RAM

[this thread](https://forum.proxmox.com/threads/proxmox-usb-bootstick-mit-log2ram-oder-folder2ram.76583/) (the one in German linked previously) mentions 'folder2ram'. After looking at both I decided 2 things:

1. I do want to move *some* additional folders to ram
2. I preferred the script used by 'log2ram' over 'folder2ram'

The folders I move to RAM (in addition to /var/log):

* '/var/lib/pve-cluster' ... Proxmox clustering info, frequently written (if services running)
* '/var/lib/rrdcached' ... Used this for UI graphs like "Summary" (possibly other places), frequently written
    + **NOTE:** after moving '/var/lib/rrdcached' to RAM I noticed gaps in graphs after rebooting.
        - This gap can be long if doing multiple reboots, 15+ minutes of missing graphs
        - I assume syncing rrdcached to disk doesn't work as fully with log2ram, I doubt this would change with folder2ram
        - Not critical, graphs update once system boots (wait a minute or two) but if this bugs you, don't move rrdcached to RAM. 
    + An alternative to moving rrdcached to RAM is to increase the time for [writes / flushs](https://forum.proxmox.com/threads/reducing-rrdcached-writes.64473/) but I haven't yet tried this. [More info](https://www.systutorials.com/docs/linux/man/1-rrdcached/)

Folders I do NOT to move to RAM, for now, that might save more writes:

* '/var/tmp' ... at least some chance that this messes with shared storage (second page of the linked forum post)
* '/var/cache' ... would likely be fine but it has the chance to get VERY big

Configuring 'log2ram' is very easy. Edit '/etc/log2ram.conf' (read the comments to get a better idea of what is going on). 

**'/etc/log2ram.conf' parameters changed:**

* from 'SIZE=40M' to 'SIZE=400M'
    + yes, 10x the original size, as I have the RAM (it's only 0.3% of my RAM) and don't want to risk overflow
    + Put this value at what you feel comfortable with on *your* system. 
* Change 'PATH_DISK="/var/log"' to 'PATH_DISK="/var/log;/var/lib/pve-cluster;/var/lib/rrdcached"'
* Change 'LOG_DISK_SIZE=100M' to 'LOG_DISK_SIZE=1000M'
    + I'm not 100% sure this needed to be done, have filed a question against it, will update depending on what I hear back. 

*Reboot now to have the changes adopted.*

What will change:

* Directories will be renamed:
    + '/var/log' > '/var/hdd.log'
    + '/var/lib/pve-cluster' > '/var/lib/hdd.pve-cluster'
    + '/var/lib/rrdcached' > '/var/lib/hdd.rrdcached'
* "New" Directories mounted on tmpfs:
    + '/var/log'
    + '/var/lib/pve-cluster'
    + '/var/lib/rrdcached'
* On boot log2ram will rsync from the renamed 'hdd.' versions into RAM
* On sync (automatically on shutdown/reboot, once per day otherwise) log2ram will rsync the tmpfs mounts to the 'hdd.' directories

### 03.C.iii. Setting swappiness

* "swappiness" is a parameter that tells Linux how much to swap RAM, including VMs/containers, to disk. 
    + Default in Proxmox VE 7.1 is 60. 
        - See the current setting via `sysctl vm.swappiness` 
        - ... or `cat /proc/sys/vm/swappiness`
    + See the current swap **usage** (likely 0 at this point) via `free`

**Important:** 'swappiness' changed in Linux kernal 3.5 (backported to 2.6.something). If searching, be aware you will find old info where "swappiness=0" behaved differently. Make sure info you use is up to date. 

***WARNING:*** **There are many considerations about changing this.** I'm not going decide for you, only show what I'v used. ***Read stuff before you do any changes.***

*Here are some places to start reading:*

*Reminder:* Ubuntu is based on Debian, as is Proxmox, don't worry about the distribution name in the links below:

* [Wikipedia](https://en.wikipedia.org/wiki/Memory_paging#Swappiness) ... intro level information
* [Ubuntu beginner info](https://help.ubuntu.com/community/SwapFaq#What_is_swappiness_and_how_do_I_change_it.3F) ... easy to read and you might want to read the whole page about swap
* [more Ubuntu info](https://dev.to/stackallflow/how-to-configure-swappiness-in-ubuntu-jkp)
* [Debian/Linux notes](https://www.mybluelinux.com/how-disable-swap-in-debian-or-linux-system/)
* [What is Swappiness On Linux?](https://www.howtogeek.com/449691/what-is-swapiness-on-linux-and-how-to-change-it/)

*I decided to try swappiness=10* and tune as needed. Swap won't be engaged until 80-90% of RAM used. 1, 5 or 20 are other options. I've seen [mentions](https://forum.proxmox.com/threads/swappiness-to-0-doesnt-seems-to-work.38439/) using 0 but I'm not comfortable with that. 

* Edit '/etc/sysctl.conf' to add "vm.swappiness = 10" to the bottom

```bash
# Copy/Paste this block into a root bash shell
FILE=/etc/sysctl.conf
cat << EOF >> $FILE

# ADDED: vm.swappiness to lower swap use, default (no value present) = 60
vm.swappiness = 10
EOF
```
* Reboot to apply change and verify with `sysctl vm.swappiness`.

**NOTES:** 

* I think I remember seeing where Proxmox can override this file with a default version. Need to keep an eye on it. Doesn't seem like this is a problem as the comments in sysctl.conf read as if editing is fine
* [4 year old issue](https://forum.proxmox.com/threads/swappiness-question.42295/) with Proxmox that I need to verify is fixed (once I have containers running). If it is, also look into [this](https://forum.proxmox.com/threads/swap-using-in-proxmox-6-2.72888/). (also: this is [Related](https://github.com/fulgerul/ceph_proxmox_scripts/blob/master/swapoff.sh))

### 03.C.iv Results

* `zpool iostat 2` shows <>500K being written once per minute to 'rpool' (the boot SSD). 
    + This is down from 1MB every few seconds. 
    + `iotop -o -a` isn't informative. 
    + Installing `fatrace` gives some information (`apt install fatrace`) but I can't see the 500K writes at all, only small writes every few seconds to '/var/lib/pve-manager' and '/var/lib/chrony'.

*This isn't a big problem.* Zero other activity would mean 150+ years to max out the warranteed 150TBW (which ends after 5 years).

---

### 03.D. Lowering ZFS RAM use

*If not using ZFS, skip this.*

**NOTE:** My system has 128GB of RAM and running multiple VMs and containers. If you have less RAM or want to emphasize ZFS performance, leave defaults and allow ZFS ARC to free memory as needed. 

By default ZFS on Proxmox uses up to 50% of RAM for ARC caching. This change lowers the amount of RAM ZFS is able to consume (64GB on my system is overkill). The reason this is optional: if you don't change this and system requests RAM for another application ZFS will release RAM from the ARC. But as I have some large VMs that I know will be frequently running, I prefer to keep that RAM open at all times. 

[Good reading](https://www.cyberciti.biz/faq/how-to-set-up-zfs-arc-size-on-ubuntu-debian-linux/)

* To see ZFS ARC min/max is *currently*:
    + `cat /proc/spl/kstat/zfs/arcstats | grep "^c_[min|max]"`
        - c_min is the minimum ARC size in bytes
        - c_max is the maximum ARC size in bytes
    + on a system with 128GB of RAM the output looks like:

<pre>c_min                           4    4218658688
c_max                           4    67498539008</pre>


On a freshly installed server without anything extra running you might not see the RAM usage. If you want to see it in action, set up a long file transfer (I copied 8TB from an old drive to my 'xfer' pool showing 64GB/50% RAM use for hours). No requirement to do this, it's just a way to see usage. 

See [this](https://www.solaris-cookbook.eu/solaris/solaris-10-zfs-evil-tuning-guide/) for Solaris ZFS background info on tuning ZFS. Don't use any commands there as they're for Solaris + ZFS (not Linux + openZFS), just good information if you want to know more about tuning ZFS. 

* ZFS specs for Solaris recommend 2GB ARC minimum
* From reading various Proxmox/Debian threads, I want minimum 4GB ARC.
* My sizing ideas ([from here](https://forums.servethehome.com/index.php?threads/how-much-ram-is-required-for-zfs.13463/) for maximum:
    + Add enough to accomodate network traffic ... 500MB ARC for each 1Gbps of bandwidth (ie, 10Gbps = 5GB)
        - currently running only 2Gbps max network (2 x interfaces connected to 1Gb switch)
        - Containers and machines using VirtIO can act like a network device with much higher throughput. 
        - 10Gbps seems good, so ... 5GB.
    + 1GB for every TB of disk in the pool to fully cache metadata
        - Currently have 18GB of usable storage. So ... 18GB.
        - But I'm going to up that over time so I'll assume 32GB. 
    + Add those up for your system and add however much more you can afford (small reads from multiple users or containers)
    + If you go over 50%, just leave things at the defaults
    + However, I feel like 37GB is higher than I need (at least for now) so I'm setting: 32GB
        - That's up to 1/4 of my RAM (from 1/2)
        - *Reminder:* ZFS will free RAM from ARC as needed
    + **IMPORTANT**: this does NOT take into account if you are going to use [ZFS deduplication](https://www.oracle.com/technical-resources/articles/it-infrastructure/admin-o11-113-size-zfs-dedup.html).
        - Add 2-3GB of RAM used for each 1TB of deduplicated storage ... I'm not using dedupe so I'm not looking up the exact amount
        - If you are going to use dedupe, consider setting it only on a ZFS dataset, not your entire pool, to minimize how much RAM it needs

* *Paste this and run it* (reminder: as root in bash)
    + If you have more or less RAM, tweak the byte values to what you want
    + You can leave out the "zfs_arc_min=" line and let ZFS decide the minimum rather than setting too low of a minimum
    + In my case the minimum I set here was the same as the default (4GB)

```bash
# Copy/Paste this block into a root bash shell
FILE=/etc/modprobe.d/zfs.conf
if test -f "$FILE"; then
    echo; echo "WARNING: $FILE exists. Edit it manually."; echo
else
    echo; echo "Creating new file: $FILE"; echo
    cat << EOF > $FILE
# ZFS ARC size
# minimum 4GiB (in bytes)
options zfs zfs_arc_min=4294967296
# maximum 32GB (in bytes)
options zfs zfs_arc_max=34359738368
EOF
    echo; echo "Done. $FILE contents:"; echo
    cat $FILE; echo
fi
```

* **Update initramfs* (only choose **one** of these options). 
    + Update ONLY the kernel currently running: `update-initramfs -u`
    + Or ... update ALL kernels installed: `update-initramfs -u -k all`
* *Reboot to apply the change*
* Login and see what your ZFS ARC is *currently* able to use
    + `cat /proc/spl/kstat/zfs/arcstats | grep "^c_[min|max]"`
        - c_min is the minimum ARC size in bytes
        - c_max is the maximum ARC size in bytes
    + After applying changes I see:

<pre>c_min                           4    4294967296
c_max                           4    34359738368</pre>

[Note](https://www.reddit.com/r/Proxmox/comments/ncg2xo/minimizing_ssd_wear_through_pve_configuration/) about monitoring writes on a daily basis, search for smartctl


---

## 03.E. Fix Missing Drivers

*Optional.*

On Debian Bullseye's initial release I had a few drivers that required the 'non-free' firmware. With Proxmox VE 7.1 I didn't have much to fix. So this is a short list and only the first is actually fixed.

With `dmesg` I see 3 items to try to fix ("{snip}" means I cut out other lines):

> {snip}
> [   13.803059] cfg80211: failed to load regulatory.db
> {snip}
[   13.875426] Bluetooth: hci0: Failed to load Intel firmware file intel/ibt-20-1-3.sfi (-2)
> {snip}
[   13.922657] thermal thermal_zone2: failed to read out thermal zone (-61)
> {snip}


### 03.E.i. 'regulatory.db'

*Error being fixed:*

> [   13.803059] cfg80211: failed to load regulatory.db

This related to wifi drivers and apparently a long-standing issue [as per this](https://kernel.googlesource.com/pub/scm/linux/kernel/git/sforshee/wireless-regdb/+/refs/heads/master).

```bash
# Copy/Paste this block into a root bash shell
mkdir tempdownload
cd tempdownload
wget https://kernel.googlesource.com/pub/scm/linux/kernel/git/sforshee/wireless-regdb/+archive/refs/heads/master.tar.gz
tar zxvf master.tar.gz
mv regulatory.db /lib/firmware
mv regulatory.db.p7s /lib/firmware
cd ..
rm -rf tempdownload
```

Fixed (after rebooting ... no need to do it right now), `dmesg | grep regulatory` shows this:

> [   13.680561] cfg80211: Loading compiled-in X.509 certificates for regulatory database

### 03.E.ii. FAILED fixes for 'iwlwifi' and 'thermal_zone2'

**WARNING: This is** ***NOT WORKING*** **as of this writing.** 

2 errors that, after investigating, I'm not going to try and fix. I'm leaving this info in case I look at it in the future. I don't  have a need for Bluetooth on the host (might pass through to a VM later).

**NOTE:** again, the following didn't fix anything, it is just a log of what was looked at. 

*Error being investigated:*

[   13.875426] Bluetooth: hci0: Failed to load Intel firmware file intel/ibt-20-1-3.sfi (-2)

Part of the "non-free" drivers. If not using Bluetooth on the host, ignore. 

**WARNING:** To get the package on a stock Debian system, modify '/etc/apt/sources.list' and add "non-free" to the Debian repos. *Doing that on Proxmox was almost a really bad mistake:* `apt update && apt upgrade` gave this after adding "non-free":

> {snip}
> The following packages will be REMOVED:
>  proxmox-ve pve-firmware pve-kernel-5.13
> The following NEW packages will be installed:
>  firmware-iwlwifi
> {snip}

**don't do that!**

Next try: install [this package](http://ftp.debian.org/debian/pool/non-free/f/firmware-nonfree/firmware-iwlwifi_20210818-1_all.deb) with `dpkg -i` (which would not get automatic updates) ... but:

> dpkg: regarding firmware-iwlwifi_20210818-1_all.deb containing firmware-iwlwifi:
> pve-firmware conflicts with firmware-iwlwifi
>  firmware-iwlwifi (version 20210818-1) is to be installed.
> 
> dpkg: error processing archive firmware-iwlwifi_20210818-1_all.deb (--install):
> conflicting packages - not installing firmware-iwlwifi
> Errors were encountered while processing:
> firmware-iwlwifi_20210818-1_all.deb

So, **same problem**.

[This Debian bug report](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=975726) (for a different iwlwifi different device) looks like Debian has updated the driver but Proxmox hasn't. 

**WARNING:** This file won't auto-update by system updates (unless added to a future version of the Proxmox firmware package). 

```bash
# Copy/Paste this block into a root bash shell
wget https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/intel/ibt-20-1-3.ddc -O /usr/lib/firmware/intel/ibt-20-1-3.ddc
wget https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/intel/ibt-20-1-3.sfi -O /usr/lib/firmware/intel/ibt-20-1-3.sfi
```

***At this point after a reboot, get an infinite loop on the console with Bluetooth disconnect/failed/reset errors. Not pursuing further. ***

...

*Error being investigated:*

> [   13.922657] thermal thermal_zone2: failed to read out thermal zone (-61)

Asked about this here: [Proxmox forums](https://forum.proxmox.com/threads/trying-to-fix-thermal-thermal_zone2-failed-to-read-out-thermal-zone-61.108064/).

`grep . /sys/class/thermal/thermal_zone2/type` reports: 

> iwlwifi_1

Appears related to the same driver that caused Bluetooth to not load. *Since this is not an important item for me, I'm not going to worry about addressing it.* Just wanted to be sure it wasn't a motherboard sensor. 

---
> [^ [TOP OF PAGE](#proxmox-ve-71-nas-and-gaming-vms---03---proxmox-tweaks)] ... ***End:*** *Proxmox VE 7.1 NAS and Gaming VMs - Proxmox Tweaks*
> 
> \> NEXT: [04 - Proxmox Extras](04.ProxmoxExtras.md)
>
> \< PREV: [02 - Proxmox Install](02.ProxmoxInstall.md)
