# Proxmox VE 7.1 NAS and Gaming VMs - 03 - Proxmox Tweaks

Various changes I make as soon as installation is done. 

## Table of Contents:
> **Hint:** Github has a drop-down automatic TOC, find the **â‰¡** icon on the top-left

* ^ [Introduction](00.Introduction.md)  (*links to* ***other pages***)
* > 03 - Proxmox Tweaks (***this page***)
    + [03.A. UI Adjustments](#03a-ui-adjustments)
    + [03.B. System Setup](#03b-system-setup)
        - [03.B.i Free/non-Subscription Repo](#03bi-freenon-subscription-repo)
        - [03.B.ii System Update](#03bii-system-update)
    + [03.C. Minimizing SSD Wear](#03c-minimizing-ssd-wear)
        - [03.C.i. Disable Proxmox High Availability Services](#03ci-disable-proxmox-high-availability-services)
        - [03.C.ii. log2ram - Move Frequently Written Files to RAM](#03cii-log2ram---move-frequently-written-files-to-ram)
            * [03.C.ii.a. initial install](#03ciia-initial-install)
            * [03.C.ii.b. moving other stuff to RAM](#03ciib-moving-other-stuff-to-ram)
        - [03.C.iii. Setting swappiness](#03ciii-setting-swappiness)
        - [03.C.iv. Lowering ZFS RAM use](#03civ-lowering-zfs-ram-use)
    + [03.D Importing Existing ZFS Pool](#03d-importing-existing-zfs-pool)
    + [03.E. Fix Missing Drivers](#03e-fix-missing-drivers)
        - [03.E.i. 'regulatory.db'](#03ei-regulatorydb)
        - [03.E.ii. FAILED fixes for 'iwlwifi' and 'thermal_zone2'](#03eii-failed-fixes-for-iwlwifi-and-thermal_zone2)
---

## 03.A. UI Adjustments

**NOTES:**

*These are* ***entirely optional*** and modify system files in a way that can be overridden in Proxmox updates. 
    + In which case you may need to re-apply them (I had to do just that after my first system update).
    + To be 100% safe you can revert these prior to applying updates but that shouldn't be needed. 
* I just do these first thing as it makes doing the rest of the admin tasks a bit nicer for me. 

* *Dark Mode*
    + There isn't an official way to get Dark Mode on your Proxmox UI
    + [PVEDiscordDark](https://github.com/Weilbyte/PVEDiscordDark) works very well and can be easily uninstalled if you don't like it. Use the instructions there, including a shift+reload in your browser, and come back here. 

* *Remove the "No Subscription" popups*
    + Honestly these are pretty mild popups, but just like Dark Mode, you likely already know you don't have a subscription and if you're using this guide you likely won't be getting one (though it's really nice to me to know I can in the future if needed). 
    + [Use Dannyda's script](https://dannyda.com/2020/05/17/how-to-remove-you-do-not-have-a-valid-subscription-for-this-server-from-proxmox-virtual-environment-6-1-2-proxmox-ve-6-1-2-pve-6-1-2/)
        - There are multiple versions listed, use the one for 7.1 (the last one, in chronological order)
        - This creates a backup of the original file. To revert the change just do this: `mv /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js.backup /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js && systemctl restart pveproxy.service` (shift+reload your browser to load the change)
    + ~~Follow the instructions on [John's Computer Services](https://johnscs.com/remove-proxmox51-subscription-notice/).~~  ... for now don't use this one as it blocks some UI elements. I've reported the problem to him. 
    + Alternately if you want to keep this change active through updates, I found [PVE Nag Buster](https://github.com/foundObjects/pve-nag-buster/) from this [Reddit thread](https://www.reddit.com/r/Proxmox/comments/tgojp1/removing_proxmox_subscription_notice/). I've chosen to use the simple edit from Dannyda above for now as I don't want the additional package installed. Your choice may vary. 

---

## 03.B. System Setup

These are ***recommended*** changes and some may be critical. 

### 03.B.i Free/non-Subscription Repo

* Navigate to 'Datacenter' > [your Proxmox host name] > 'Updates' > 'Repositories'
    + **No need to do this if you DO have a subscription. But for the home user this is needed to get updates running.** 
    + click the 'Add' button and add the "No-Subscription" repo
    + click the line for "https://enterprise.proxmox.com/debian/pve" (components has "pve-enterprise") and then click the 'Disable' button

---

### 03.B.ii System Update

* 'Datacenter' > [your machine] > 'Updates'
    + 'Refresh' (and click the top-right 'X' when you see "TASK OK")
    + '>_ Upgrade'
        - opens a window with a shell
        - if all looks good, hit [enter] to accept the 'Y/n' prompt
        - let it run
            * it's done when you see the shell prompt return
            * look to see if the last message recommends a reboot
            * close the shell window
        - if it recommended reboot, do so (shutdown VMs/etc if running) with the 'Reboot' button in the UI
        - if this included a kernel update
            * you will see multiple kernels to select on boot
            * default is going to be the kernel you just installed

---

## 03.C. Minimizing SSD Wear

Proxmox is built with the assumption of enterprise-class hardware. That doesn't mean it won't work just fine for a consumer-class PC in a home environment. But it does mean that Proxmox has a history of putting heavy usage on it's boot drive. If your boot drive is a consumer SSD and you don't modify things it can wear that device out very quickly (months from 0% to 100% wear). 

This is made worse by the fact that consumer devices vary widely from use of SLC (rare these days, but expected in enterprise SSDs), MLC, TLC, QLC, PLC, etc flash ram. A SLC drive only writes 1 bit per cell, MLC does 2, TLC 3, QLC 4, PLC 5. This increases the amount of storage per cell but each increase lowers the overall lifetime of that cell. [More information on this](https://www.howtogeek.com/444787/multi-layer-ssds-what-are-slc-mlc-tlc-qlc-and-mlc/). 

TLDR; A general rule is ... the cheaper the drive is per TB, the less it can endure writing data. And Proxmox can write a lot of data. 

### 03.C.i. Disable Proxmox High Availability Services

**This is only for a single-node Proxmox install (ie, you're not clustering multiple devices that run Proxmox).** However you should be able to enable these services later on if you decide to use clustering. These services do a lot of constant writes. Hopefully future versions of Proxmox will look at giving a UI option to write more of this data to RAM, but since we don't *need* them for this type of install, they're going to be turned off. 

```bash
systemctl disable pve-ha-lrm
systemctl disable pve-ha-crm
systemctl disable corosync.service
systemctl disable pvesr.timer
```

**NOTES:**

* `systemctl disable pvesr.timer` gave an error, likely as I had never configured Replication, so don't worry about the message. 
* If you wanted to keep clustering services active but minimize SSD wear, you could look into moving /var/lib/pve-cluster into log2ram. [Information about this](https://forum.proxmox.com/threads/proxmox-usb-bootstick-mit-log2ram-oder-folder2ram.76583/) (in German but Google Translate does a good job). I've included what I did for this in the information below.  

### 03.C.ii. log2ram - Move Frequently Written Files to RAM

[log2ram](https://github.com/azlux/log2ram) creates a ramdisk and mounts it at '/var/log', then will perform a copy daily of the data to a specified drive. If the system is gracefully shudown it will also copy the log to disk.

* Pro: saves the constant writing to log files during normal operation
* Con: **an ungraceful shutdown, including a kernel panic, won't flush the log to disk*

So decide which is more important to you. You can read more about it [here](https://linuxfun.org/en/2021/01/01/what-log2ram-does-en/)

Click the links above to read more about the process and install. My steps are just going to be what I ran to implement the install from the readme (since I log in as root, the `sudo` in the readme version is removed)

**NOTES:** 

* The README for log2ram mentions a config option for 'RSYNC', this is an older version option. Current version of log2ram will just use rsync if it is available, which it is by default on Proxmox VE. I've filed and issue for the maintainer to look at the README contents. 
* The config talks about ZRAM / ZL2R ... this appears to be for raspberry pi systems and I haven't used it. 

#### 03.C.ii.a. initial install

```bash
echo "deb [signed-by=/usr/share/keyrings/azlux-archive-keyring.gpg] http://packages.azlux.fr/debian/ bullseye main" | tee /etc/apt/sources.list.d/azlux.list
wget -O /usr/share/keyrings/azlux-archive-keyring.gpg  https://azlux.fr/repo.gpg
apt update
apt install log2ram
```

At this point if you only want to have the logs in RAM, you can reboot and call this one done. But read the next note before rebooting. 

#### 03.C.ii.b. moving other stuff to RAM

On reading [this thread](https://forum.proxmox.com/threads/proxmox-usb-bootstick-mit-log2ram-oder-folder2ram.76583/) (German, use translate as necessary) and comparing 'log2ram' vs 'folder2ram' I decided 2 things:

1. I wanted to move additional folders to ram
2. I preferred the script used by 'log2ram' over 'folder2ram'

The folders I wanted to move to RAM (in addition to /var/log):

* 'tmpfs /var/lib/pve-cluster' ... Proxmox clustering info, frequently written (if the service is running)
* 'tmpfs /var/lib/rrdcached' ... Proxmox uses this for the graphs in the UI (and possibly other places), frequently written

The folders I decided NOT to move to RAM for now but that might save more writes:

* '/var/tmp' ... at least some chance that this messes with shared storage (second page of the linked forum post)
* '/var/cache' ... would likely be fine but it has the chance to get VERY big

Configuring 'log2ram' for this is very easy. Edit the file '/etc/log2ram.conf'. When you edit it you can read the comments to get a better idea of what is going on. 

**'/etc/log2ram.conf' parameters changed:**

* from 'SIZE=40M' to 'SIZE=400M'
    + yes, I gave it 400M of space for these files even though at least initially they are nowhere near that large.
    + Remember that I'm running 128GB of RAM, so that's 0.3% of the total. 
    + Put this value at what you feel comfortable with on *your* system. 
* from 'PATH_DISK="/var/log"' to 'PATH_DISK="/var/log;/var/lib/pve-cluster;/var/lib/rrdcached"'
* from 'LOG_DISK_SIZE=100M' to 'LOG_DISK_SIZE=1000M'
    + I'm not 100% sure this needed to be done, have filed a question against it, will update depending on what I hear back. 

Reboot now to have the changes adopted.

What will change on your system:

* The following directories will be renamed:
    + '/var/log' > '/var/hdd.log'
    + '/var/lib/pve-cluster' > '/var/lib/hdd.pve-cluster'
    + '/var/lib/rrdcached' > '/var/lib/hdd.rrdcached'
* The following will be mounted to tmpfs:
    + '/var/log'
    + '/var/lib/pve-cluster'
    + '/var/lib/rrdcached'
* On boot log2ram will rsync from the renamed 'hdd.' versions into RAM
* On sync (which happens automatically on shutdown/reboot) log2ram will rsync the RAM contents to the 'hdd.' directories

### 03.C.iii. Setting swappiness

### 03.C.iv. Lowering ZFS RAM use

*If you're not using ZFS, skip this.*

**NOTE:** My system has 128GB of RAM and will be running multiple VMs and containers. If your system has less RAM or you want to emphasize ZFS performance, just leave it at the default and allow ZFS ARC to free memory as needed. 

By default ZFS on Proxmox will use up to 50% of RAM for ARC caching. This will let you lower the amount of RAM ZFS is able to consume (64GB on my system is quite overkill). One thing to make you feel better is if the system requests RAM for another application then ZFS will free up it's ARC. But having RAM simply available all the time is more performant. 

* The ZFS specs for Solaris say 2GB is the minimum
* Based on reading various Proxmox/Debian threads, I'd want minimum 4GB ARC for a system with just a couple of drives
* Some general ideas ([many from here](https://forums.servethehome.com/index.php?threads/how-much-ram-is-required-for-zfs.13463/):
    + Add enough to accomodate network traffic ... 500MB ARC for each 1Gbps of bandwidth (ie, 10Gbps = 5GB)
        - I'm currently running only 2Gbps max network (2 x interfaces connected to 1Gb switch)
        - Containers and machines using VirtIO can act like a network device with much higher throughput. 
        - I'm going to set for 10Gbps needed. So ... 5GB.
    + 1GB for every TB of disk in the pool to fully cache metadata
        - I currently have 18GB of usable storage. So ... 18GB.
        - But I'm going to up that over time so I'll assume 32GB. 
    + Add those up and add however much more you can afford (small reads from multiple users or containers)
    + However, I feel like 37GB is still really high and these are probably ok to mix together a bit. Final amount I'm setting: 32GB
        - That's up to 1/4 of my RAM (fown from 1/2)
        - Reminder: ZFS will free RAM from ARC as needed
        - My main gaming VM will get 32GB (yes, my main game actually needs it, Star Citizen loves RAM)
        - That will leave 32GB free at all times for other containers and VMs with up to 64GB if things get busy
    + **IMPORTANT**: this does NOT take into account if you are going to use ZFS deduplication.
        - Add 2-3GB of RAM used for each 1TB of deduplicated storage
        - If you are going to use dedupe, consider setting it only on a ZFS dataset, not your entire pool, to minimize how much RAM it needs



---

## 03.D. Importing Existing ZFS Pool

*If you're not using ZFS, skip this OR if you don't have any ZFS pools from previous installs.* However it may still be useful for you later on if you're reinstalling Proxmox and want to keep your old pools. 

I created a ZFS striped pool with my 2 4TB "scratch" disks on a previous TrueNAS Scale system that has data I want to access now (it has my .iso files and support utilities). It's very easy to import this existing pool into Proxmox. As long as you remember the name of the pool. If you don't, the following won't help you figure it out but you should be able to search for that information. 

* Open a root shell 
    + if you have a way to SSH from a terminal, do that via `ssh root@[yourhostip]` ... switching views in the Proxmox UI shell will keep closing the shell
    + if you don't, then you can use the web shell, just make sure you're done with what you're working on before clicking out of it each time (or open a new tab just for the shell)
* `zpool import [poolname]`
    + ie, in my case the pool name is 'xfer', so the command would be `zpool import xfer`
    + if this command complains that the pool was in use before (mine was created on a TrueNAS machine), add -f like: `zpool import -f xfer`
* At this point the pool should be mounted at the filesystem root (ie, "/xfer") and visible in the Proxmox UI. I am not doing any configuration of the mount point or parameters as this is just being used to access files temporarily until I can move them to their final resting place. 

---

## 03.E. Fix Missing Drivers

*Optional.*

On Debian Bullseye's initial release I had a few drivers that weren't installed. With Proxmox VE 7.1 I didn't have much to fix. I don't know whether it was Proxmox or Debian that fixed these in a more recent update but I'll take it. 

Sorting through the output of `dmesg` I see these 3 items I want to fix ("{snip}" just means I cut out other lines):

> {snip}
> [   13.803059] cfg80211: failed to load regulatory.db
> {snip}
[   13.875426] Bluetooth: hci0: Failed to load Intel firmware file intel/ibt-20-1-3.sfi (-2)
> {snip}
[   13.922657] thermal thermal_zone2: failed to read out thermal zone (-61)
> {snip}


### 03.E.i. 'regulatory.db'

*Error being fixed:*

> [   13.803059] cfg80211: failed to load regulatory.db

This is part of the wifi drivers working and apparently a fairly long-standing issue [as per this](https://kernel.googlesource.com/pub/scm/linux/kernel/git/sforshee/wireless-regdb/+/refs/heads/master).

```bash
mkdir tempdownload
cd tempdownload
wget https://kernel.googlesource.com/pub/scm/linux/kernel/git/sforshee/wireless-regdb/+archive/refs/heads/master.tar.gz
tar zxvf master.tar.gz
mv regulatory.db /lib/firmware
mv regulatory.db.p7s /lib/firmware
cd ..
rm -rf tempdownload
```

Fixed (after rebooting ... no need to do it right now), `dmesg | grep regulatory` shows this:

> [   13.680561] cfg80211: Loading compiled-in X.509 certificates for regulatory database

### 03.E.ii. FAILED fixes for 'iwlwifi' and 'thermal_zone2'

These are 2 errors that, after investigating, I'm not going to try and fix. 

**WARNING: This is** ***NOT WORKING*** **as of this writing.** 

I'm leaving the notes up in case I look at it in the future. But since I don't really have a need for Bluetooth on the Proxmox host (it might get passed through to a VM later which won't need the driver on the host) I'm not pursuing further. . 

*Error being investigated:*

[   13.875426] Bluetooth: hci0: Failed to load Intel firmware file intel/ibt-20-1-3.sfi (-2)

This is part of the "non-free" drivers. And if you're not using Bluetooth on this machine it is *optional*. 

**WARNING:** To get the package on a stock Debian system you would modify '/etc/apt/sources.list' to add the "non-free" tag to your Debian repos. Doing that on Proxmox was almost a really bad mistake with `apt update && apt upgrade` giving this message after adding "non-free":

> {snip}
> The following packages will be REMOVED:
>  proxmox-ve pve-firmware pve-kernel-5.13
> The following NEW packages will be installed:
>  firmware-iwlwifi
> {snip}

No, we **don't want that!**

Next we might try to just install [this package](http://ftp.debian.org/debian/pool/non-free/f/firmware-nonfree/firmware-iwlwifi_20210818-1_all.deb) with `dpkg -i` (which would mean we won't get future updates unless we do it manually again) ... but ... *we get this:*

> dpkg: regarding firmware-iwlwifi_20210818-1_all.deb containing firmware-iwlwifi:
> pve-firmware conflicts with firmware-iwlwifi
>  firmware-iwlwifi (version 20210818-1) is to be installed.
> 
> dpkg: error processing archive firmware-iwlwifi_20210818-1_all.deb (--install):
> conflicting packages - not installing firmware-iwlwifi
> Errors were encountered while processing:
> firmware-iwlwifi_20210818-1_all.deb

So basically **the same problem**

Thanks to [this Debian bug report](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=975726) (which is for a different device but has what we need to move further) it looks like Debian has updated the driver but Proxmox hasn't. Maybe it's not been stability tested yet. *You can still stop.* But I'm going to go ahead and fix it for my system. 

**WARNING:** This file won't be updated by system updates (unless it gets added to a future version of the firmware package). 

```bash
wget https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/intel/ibt-20-1-3.ddc -O /usr/lib/firmware/intel/ibt-20-1-3.ddc
wget https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/tree/intel/ibt-20-1-3.sfi -O /usr/lib/firmware/intel/ibt-20-1-3.sfi
```

***At this point after a reboot I get an infinite loop on the console with Bluetooth disconnect/failed/reset errors. Not pursuing further. ***

*Error being investigated:*

> [   13.922657] thermal thermal_zone2: failed to read out thermal zone (-61)

After asking about this on the [Proxmox forums](https://forum.proxmox.com/threads/trying-to-fix-thermal-thermal_zone2-failed-to-read-out-thermal-zone-61.108064/) it appears this is related to the same driver that causes Bluetooth to not load. 

`grep . /sys/class/thermal/thermal_zone2/type` reports: 

> iwlwifi_1

Since this is not an important item for me, I'm not going to worry about addressing it. 

---

---

### Useful Utilities

* `iotop`
  + to see what is actively using system IO
  + `apt install iotop`


---
> [^ [TOP OF PAGE](#proxmox-ve-71-nas-and-gaming-vms---03---proxmox-tweaks)] ... ***End:*** *Proxmox VE 7.1 NAS and Gaming VMs - Proxmox Tweaks*
> 
> \> NEXT: [04 - Replace Me](03.ReplaceMe.md)
>
> \< PREV: [02 - ProxmoxInstall](02.ProxmoxInstall.md)
Other stuff: Windows on USB for firmware/benchmarks/etc
