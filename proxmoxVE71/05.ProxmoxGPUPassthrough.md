# Proxmox VE 7.1 NAS and Gaming VMs - 05 - Proxmox GPU Passthrough

***This file is under heavy revision and not currently usable***

This is how I configure my system for GPU Passthrough. It is a bit different than some other guides (no blacklisting, 'driverctl' instead). 

## Table of Contents:
> **Hint:** Github has a drop-down automatic TOC, find the **â‰¡** icon on the top-left

* ^ [README](README.md)  (*links to* ***other pages***)
* 05 - Proxmox GPU Passthrough (***this page***)
    + [05.A. VFIO Kernel Modules](#05a-vfio-kernel-modules)
    + [05.B. Boot Parameters](#05b-boot-parameters)
    + [05.C. GPU IDs](#05c-gpu-ids)
    + [05.D. driverctl Hookscript](#05d-driverctl-hookscript)
        - [05.D.i. Override GPU Modules](#05di-override-gpu-modules)
        - [05.D.ii. Reverting 'drivertctl' Overrides](#05dii-reverting-driverctl-overrides)
        - [05.D.iii. Switch Between Console Video and VFIO](#05diii-switch-between-console-video-and-vfio)
* \> [06 - VM Windows 10](06.VMWindows10.md) (*next page*)

**NOTES:**

* In BIOS settings on [01 - Hardware](01.Hardware.md) I made a note about setting my boot GPU device (console video) to PCIE Slot 2, this is where knowing this starts to matter
* **Not meant to be a universal guide**, this page has specifics for my AMD Ryzen + Nvidia GPUs. But *can be adapted to other setups*
* This will be useful for both single GPU, dual GPU with 1 used for the host, and dual passthrough GPU configs
* I used ZFS on the host boot disk (rpool). If using grub there is extra info but I can't test it

---

## 05.A. VFIO Kernel Modules

* **NOTE:** other Linux distributions may have these already built in to the kernel, but Proxmox still uses these as modules
    + Verify this via `cat /boot/config-5.13.19-6-pve |grep -i vfio` (update the name of 'config' for the kernel installed, it will change on updates)
    + Just something to be aware of as guides in the future will likely start omitting this step as it becomes unnecessary
    + Add these lines to '/etc/modules':
        - vfio
        - vfio_iommu_type1
        - vfio_pci
        - vfio_virqfd

```bash
# Copy/Paste this block into a root bash shell
FILE=/etc/modules
cat << EOF >> $FILE

vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd

EOF
echo; echo "Done. $FILE contents:"; echo
cat $FILE; echo
``` 

---

## 05.B. Boot Parameters

This is where you may have some trial and error. 

*Reminder:* My system has 2 GPUs in the first 2 PCIE slots. The first PCIE slot has my "big" GPU, the second PCIE slot has the smaller GPU. I want to keep the big GPU free for VM use. If you have a different configuration, *some* of the steps below will be different. 

**NOTE:** 'VT' below stands for 'Virtual Terminal'. This is what provides the text login screen and allows you to use a keyboard/mouse on the host without a UI. Aka "console" in a lot of other pages. 

* Add kernel boot parameters (if you have other options on your cmdline, add the following, don't replace the other items)
    + The place you will add these depends on whether you use systemd or grub to boot
    + The string to add: `quiet amd_iommu=on iommu=pt`
        - `quiet` ... leave this out *If you want to see detailed boot info*, using it speeds up boot at the expense of info
        - `amd_iommu=on` is **critical** to following steps working
        - `iommu=pt` may not be required
        - You can also add `fbcon=map:1` to the above if you have 2 GPUs and want VT monitor output on the second GPU (allows VT to be active while the big GPU is used by a VM, is temporarily disabled when using the small GPU with a VM).  *Leaving this out isn't a problem*, the VT will be active on the first GPU and go away when used by a VM (but returns after VM is shut down). 
          Example:  
	  > `quiet amd_iommu=on iommu=pt fbcon=map:1`

+ systemd boot:
    + Add the above to '/etc/kernel/cmdline':  
    + Example with minimum added:  
        > root=ZFS=rpool/ROOT/pve-1 boot=zfs quiet amd_iommu=on iommu=pt  
      (I also add "fbcon=map:1" to mine)
* grub boot:
    + Add to the CMDLINE in '/etc/default/grub'
    + Example with minimum added:  
	    > GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on iommu=pt"  
      (I also add "fbcon=map:1" to mine)
 
 * Apply the above changes:
    + `proxmox-boot-tool refresh`
    + **Reboot now** to apply the change

* ***Extra:*** *if you have problems attaching to the GPUs* you can test the following additions to the cmdline (above)
    + On my system it hasn't been needed
    + Don't add these until you need to try to fix something
    + Try each individually to see if only 1 will fix it
    + *Ideally you won't need any of these, giving you the best chance to have VT active if needed.*
        - `nomodeset` ... don't change display modes
        - `nofb` ... disable framebuffer on the VT
        - `video=efifb:off` ... disable framebuffer for EFI
        - `video=vesafb:off,efifb:off` ... disable VESA modes AND framebuffer for EFI
        - Example including ALL of the above (you can add all to see if your system boots and attaches GPUs properly, then test removing some):
            > `quiet amd_iommu=on iommu=pt fbcon=map:1 nomodeset nofb video=vesafb:off,efifb:off`

---

## 05.C. GPU IDs

Use `iommu_list` (from [04 - Proxmox Extras](04.ProxmoxExtras.md#04b-useful-utilities)) to see the GPU device IDs:

*Example of `iommu_list` for my GPUs:*

<pre>
{snip}
IOMMU Group 28:
	0b:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:2208] (rev a1)
	0b:00.1 Audio device [0403]: NVIDIA Corporation GA102 High Definition Audio Controller [10de:1aef] (rev a1)
IOMMU Group 29:
	0c:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 6GB] [10de:1c03] (rev a1)
	0c:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1)
{snip}
</pre>

* Modern GPUs have at least 2 device IDs (video and HDMI audio)
    + Your GPU may have more devices (USB, etc)
    + Very old GPUs may only have video
* My system has 2 different GPU models, makes it easy to know which is on each PCI id. 
    + RTX 3080TI (first PCIE slot)
        - First (0b:00:0) is the video output
        - Second (0b:00.1) is the audio output
    + GTX 1060 6GB (second PCIE slot)
        - First (0c:00:0) is the video output
        - Second (0c:00.1) is the audio output
* Your device IDs will likely be different, ie '02:00.0' instead of '0b:00.0', etc.
* * For use with 'driverctl' add '0000:' to those IDs.    
     **Example:** '0b:00.0' becomes '0000:0b:00.0'.

---

## 05.D. driverctl Hookscript

This section is for passing through PCI devices using a 'hookscript'. 

This allows **all** GPUs to be used for VMs *but* return Virtual Terminal video output when that GPU is not claimed by a VM. 

Not needed *If you are are* ***dedicating a separate GPU for console output*** *and/or won't want to switch VMs often.* If you are doing single GPU passthrough (or using all GPUs for passthrough) then this process is useful. 

Feel free to test your VM (see [06 - VM Windows 10](06.VMWindows10.md) for creating one) before coming back here if you aren't sure you want to add this. 

### 05.D.i. driverctl Install

 'driverctl' is in the Debian Bullseye repository. It can ***override*** a GPU temporarily *instead of blacklisting modules*. It can reassign devices, repeatedly, without rebooting **while giving virtual terminal video** when a VM isn't active on that GPU ([originally found here](https://www.heiko-sieger.info/blacklisting-graphics-driver/)). 

This is needed if using the hookscript in this page.
 
**NOTES:** 
* *If you override the GPU used for the virtual terminal,* ***video output for the virtual terminal will stop while the VM is using the GPU.***

* `apt install driverctl`
* `driverctl list-devices` ... shows which devices are on what modules currently. Nvidia GPUs will look like this (AMD and Intel GPUs will look similar with different module names):  
    > 0000:0c:00.0 nouveau  
    > 0000:0c:00.1 snd_hda_intel  
* `driverctl list-overrides` ... shows any overrides set (empty for now, ignore error)

### 05.D.ii. Create Hookscript

This "hookscript" runs automatically to manage 'driverctl' whenever the VM is started or stopped. 

**More Hookscript Information:**

* [Proxmox docs](https://pve.proxmox.com/pve-docs/pve-admin-guide.html#_hookscripts)
* [Proxmox - Execute Script on Host](https://blog.infoitech.co.uk/proxmox-execute-script-on-host/)
* [Proxmox PCI Switcher](https://github.com/rosineygp/proxmox-pci-switcher) ... a Python *alternative* to my hookscript. 
    + *PCI Switcher automatically shuts down a running VM using a PCI to start a new VM that needs the same PCI device*
    + ***My script (below) purposefully fails to start a VM if a needed PCI device is in use***, avoiding the chance of losing work in that VM
    + Figure out which method you want
        - If you prefer the PCI Switcher method much of the information below will be the same but you'll need to adapt the directions.
        - Skip the information below related to 'driverctl' if you go with PCI Switcher

**Create the hookscript:**

* Edit '/etc/pve/storage.cfg'
    + find:  
        >        content iso,vztmpl,backup
    + add ",snippets", like:  
        >        content iso,vztmpl,backup,snippets
    + save the file
    + Shift+Reload the *Proxmox web UI*
        - Proxmox UI will add a new item: 'Datacenter' > '[your host]' > 'local (your host)' (storage) > 'Snippets' (middle column)
        - Proxmox will create a file directory '/var/lib/vz/snippets'
        - *note:* You could make the directory in shell, but you also want Proxmox to know about the directory
* ***Open [hookscript-driverctl.pl](hookscript-driverctl.pl)***  *in a new browser window*
     + Copy the contents of that script (it's long, not embedded here)
     + Paste the contents into **'/var/lib/vz/snippets/hookscript-driverctl.pl'
     + `chmod +x /var/lib/vz/hookscript-driverctl.pl`

### 05.D.iii. Attach Hookscript

**NOTE:** This won't be available until you have created a VM (example on [06.VM Windows 10](06.VMWindows10.md)). If you haven't done that yet, just come back here when ready. 

* Determine the <vmid> for the passthrough VM (the first VM you created will be '100' unless you changed that manually, see it in the UI)
* In a shell do this: `qm set <vmid> -hookscript local:snippets/hookscript-driverctl.pl`  
    Example: `qm set 100 -hookscript local:snippets/hooksciprt-driverctl.pl`
* At this point hookscript will be active:
    + 'pre-start' ... sets overrides on PCI devices before starting the VM (but after you press the UI 'Start' button)
    + 'post-stop' ... unsets overrides after the VM has been shut down via the UI 'Shutdown' button

**NOTES:**

* Hookscripts can be in different languages
    + Proxmox docs example is in Perl and I was already comfortable with that
    + Python is used by the PCI Switcher script linked above
    + Bash and other languages also work
* Hookscripts can also be activated during the 'post-start' phase (runs once the VM is running) and 'pre-stop' phase (runs just before shutting down the VM). My hookscript doesn't need to do anything in those phases
* VMs can only have a single hookscript .... *If you want to do additional actions*:
    + Copy 'hookscript-driverctl.pl' to a new file like 'my-hookscript.pl' (whatever name you want), avoids breaking other VMs still using the original
     + Add your custom commands to the new file (see comments in my hookscript for info on how to debug yours)
     + Attach the new hookscript via:
         `qm set <vmid> -hookscript local:snippets/my-hookscript.pl`
* Multiple VMs can use the same hookscript
    +Attach the same script to the different <vmid> using the command above
    + Remove a hookscript from a VM via `qm set <vmid> --delete hookscript`

* *Promox UI and Hookscripts:*
    + You can *see which hookscript the VM has* in the 'Options' for that VM in the Proxmox UI, but you ***can't change it from the UI*** (see last few lines above for how to modify the hookscript or remove it from the VM)
    + In the Proxmox UI 'Tasks' / 'Task History'
        - See what happened with the hookscript *in the 'pre-start' phase*  (Task: "VM <vmid> - Start")
        - Can't see what happens ***in the 'post-stop'*** phase 

* *Syslog:*
    + For 'hookscript-driverctl.pl' I've added code to send output to syslog as well so that you can see 'post-start' information.
        - Proxmox UI ... use the 'Syslog' of the host in UI
        - Via shell:  
            `grep hookscript-driverctl.pl /var/log/syslog`
    + If debugging gets hard, you can start/stop the VM from shell (see [05.E.i](#05ei-stop-and-start-vm-from-shell) below) to see 'qmeventd' events in syslog. 

---

## 05.E. Extra Information

You don't need to do anything on your host with the rest of this page. These are just pointers on how to do some related things. 

### 05.E.i. Stop and Start VM from shell

If you want to start/stop VMs in the shell:

* `qm start <vmid>` is basically the same as pressing 'Start' in the UI
* `qm stop <vmid>` is basically the same as pressing 'Shutdown' in the UI
    + if a VM is unable to shutdown, do:  
        `qm stop <vmid> -skiplock`
            - this can shutdown VMs that won't shutdown via the UI
            - *Use with caution*
* Using 'qm' above will output to syslog info that would normally be captured by the UI 'Tasks' view if using the buttons there
    + This can be useful for seeing output in syslog for 'qmeventd' that the UI doesn't capture (like in the 'post-stop' phase). 
    + I've made my hookscript send extra info to syslog, this would be more needed by other hookscripts that might not do that or to see info without using the UI while debugging. 

### 05.E.ii. How to  use driverctl manually

You don't need to manually use `driverctl` once the hookscript is running, but this section will give you information on how it works. **Don't to run these commands while setting up from this page, this is FYI stuff.**

**Bind GPU devices to the VFIO module:**

* '--nosave' tells driverctl to override now but not permanently. This allows something to go wrong and not prevent reboot from working. 
* 'set-override' has driverctl bind the device (ie, '0000:0b:00.0') to the 'vfio-pci' module
* **Edit these to match your IDs:**  
    `driverctl --nosave set-override 0000:0b:00.0 vfio-pci`  
    `driverctl --nosave set-override 0000:0b:00.1 vfio-pci`  
    `driverctl --nosave set-override 0000:0c:00.0 vfio-pci`  
    `driverctl --nosave set-override 0000:0c:00.1 vfio-pci`  
* *virtual terminal video goes blank at this point if one of those GPUs is used for it*
* If you have a passthrough VM already created, you can start it now to see if passthrough worked. 

**See device bindings:**

* `driverctl list-devices` for GPU(s) now looks like this for overridden GPUs  
    ("[\*]" means it is set via override and persists):  
> 0000:0b:00.0 vfio-pci [\*]  
> 0000:0b:00.1 vfio-pci [\*]  
> 0000:0c:00.0 vfio-pci [\*]  
> 0000:0c:00.1 vfio-pci [\*]  

**See current overrides:**

* `driverctl list-overrides` ... all current overrides  
    (no "[\*]" shown, implied by 'list-overrides'). 

**Remove overrides:**

* If you are testing the prior commands in a VM, **shut down the VM before doing this.**
* Video will bind to 'nouveau' for Nvidia GPUs, audio will either rebind to 'snd_hda_intel' or remain on 'vfio-pci' if the system wasn't using it. 
    `driverctl --nosave unset-override 0000:0b.0`  
    `driverctl --nosave unset-override 0000:0b.1`  
    `driverctl --nosave unset-override 0000:0c.0`  
    `driverctl --nosave unset-override 0000:0c.1`  
* *virtual terminal video will reappear at this point, press ENTER to see the login prompt*

**--noprobe:**

If you add the **'--noprobe'** argument (like '--nosave' shown above) the command will try to execute regardless of whether the device is actively in use. This can help if you've done something that messed up a binding. But you may still end up in a "better to just reboot" situation. That's actually the reason to use '--nosave' argument, so you can reboot without having a change that might break things.  

## 05.D.iii. Extra pointers

* One of my monitors is connected via a DVI-D connector 
    + When the virtual terminal reconnects to it after 'driverctl' unbinds 'vfio-pci' the text is garbled
    + This doesn't happen on the same monitor using Displayport on the same GPU)
    + You can help this by increasing the font size in '/etc/default/console-setup' and running `setupcon` to apply the changes
    + See what fonts are available in '/usr/share/consolefonts/'. 'Lat15-TerminusBold32x16' worked to make the text legible on the DVI output (but I actually just switched to using the Displayport)

---

At this point we are able to create a Windows 10 virtual machine with a passed through GPU {[Next Page](06.VMWindows10.md)}.

---
> [^ [TOP OF PAGE](#proxmox-ve-71-nas-and-gaming-vms---05---proxmox-gpu-passthrough)] ... ***End:*** *Proxmox VE 7.1 NAS and Gaming VMs - Proxmox GPU Passthrough*
> 
> \> NEXT: [06 - VM Windows 10](06.VMWindows10.md)
>
> \< PREV: [04 - Proxmox Extras](04.ProxmoxExtras.md)
