# Proxmox VE 7.1 NAS and Gaming VMs - 05 - Proxmox GPU Passthrough

This is how I configure my system for GPU Passthrough. It is a bit different than some other guides (no blacklisting, 'driverctl' instead). 

## Table of Contents:
> **Hint:** Github has a drop-down automatic TOC, find the **â‰¡** icon on the top-left

* ^ [README](README.md)  (*links to* ***other pages***)
* 05 - Proxmox GPU Passthrough (***this page***)
    + [05.A. VFIO Kernel Modules](#05a-vfio-kernel-modules)
    + [05.B. Boot Parameters](#05b-boot-parameters)
    + [05.C. GPU IDs](#05c-gpu-ids)
    + [05.D. Install and Configure 'driverctl'](#05d-install-and-configure-driverctl)
        - [05.D.i. Override GPU Modules](#05di-override-gpu-modules)
        - [05.D.ii. Reverting 'drivertctl' Overrides](#05dii-reverting-driverctl-overrides)
        - [05.D.iii. Switch Between Console Video and VFIO](#05diii-switch-between-console-video-and-vfio)
* \> [06 - VM Windows 10](06.VMWindows10.md) (*next page*)

**NOTES:**

* In BIOS settings on [01 - Hardware](01.Hardware.md) I made a note about setting my boot GPU device (console video) to PCIE Slot 2, this is where knowing this starts to matter.
* **Not meant to be a universal guide**, this page has specifics for my AMD Ryzen + Nvidia GPUs. But *can be adapted to other setups*.
* This will be useful for both single GPU, dual GPU with 1 used for the host, and dual passthrough GPU configs. 
* I used ZFS on the host boot disk (rpool). If using grub there is extra info but I can't test it. 

---

## 05.A. VFIO Kernel Modules

* **NOTE:** other Linux distributions may have these already built in to the kernel, but Proxmox still uses these as modules
    + Verify this via `cat /boot/config-5.13.19-6-pve |grep -i vfio` (update the name of 'config' for the kernel installed, it will change on updates)
    + Just something to be aware of as guides in the future will likely start omitting this step as it becomes unnecessary
    + Add these lines to '/etc/modules':
        - vfio
        - vfio_iommu_type1
        - vfio_pci
        - vfio_virqfd

```bash
# Copy/Paste this block into a root bash shell
FILE=/etc/modules
cat << EOF >> $FILE

vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd

EOF
echo; echo "Done. $FILE contents:"; echo
cat $FILE; echo
``` 

---

## 05.B. Boot Parameters

* Add kernel boot parameters (if you have other options on your cmdline, add the following, don't replace the other items)
    + systemd
        - Add this to '/etc/kernel/cmdline': " quiet amd_iommu=on iommu=pt"  
        Example:  
	    > root=ZFS=rpool/ROOT/pve-1 boot=zfs quiet amd_iommu=on iommu=pt
    + grub 
        - Add "quiet amd_iommu=on iommu=pt" to the CMDLINE in '/etc/default/grub'  
        Example:  
	    > GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on iommu=pt"


* Apply the above changes:
    + `proxmox-boot-tool refresh`
    + `update-initramfs -u -k all` (or `update-initramfs -u` to only update the current kernel)
        - NOTE: I'm unsure both of these are needed
            * if not, which is better?
            * Need to learn more about 'proxmox-boot-tool'
    + Reboot to apply the change

---

## 05.C. GPU IDs

Use `iommu_list` (created on [this page](04.ProxmoxExtras.md#04b-useful-utilities)) to see the GPU device IDs. My system:

* Big GPU (RTX 3080TI, first PCIE slot): '0b:00.0' (video) and '0b:00.0' (audio)
* Little GPU (GTX 1060, second PCIE slot): '0c:00.0' (video) and '0c:00.0' (audio)
* For use with the next section, add '0000:' to those. Ie, '0b:00.0' becomes '0000:0b:00.0'.
    + and adapt the next commands to match your IDs, you may have something different like '02:00.0', etc.

*Example of the output for my system config:*

<pre>
{snip}
IOMMU Group 28:
	0b:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:2208] (rev a1)
	0b:00.1 Audio device [0403]: NVIDIA Corporation GA102 High Definition Audio Controller [10de:1aef] (rev a1)
IOMMU Group 29:
	0c:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 6GB] [10de:1c03] (rev a1)
	0c:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1)
{snip}
</pre>


## 05.D. Install and configure 'driverctl'

**NOTES:**

* 'driverctl' is a part of the Debian Bullseye repositories and has worked for me on multiple Debian-based systems for many months. I use it *instead of blacklisting*. Allows reassigning devices repeatedly and use modules as needed vs blacklisting and disabling those modules ([originally found here](https://www.heiko-sieger.info/blacklisting-graphics-driver/)).
* *If you only have 1 GPU device and override,* ***video output to your console monitor will stop***, but you can get it back as needed using commands shown below. 
     + I have 2 GPUs and overriding both below, which will also disconnect console output. 
     + These are different GPU models ... I don't need to worry about having 2 identical devices. **If you do have 2 identical models** you may need to consult another guide to fix issues (but the following may just work, I can't test)
* *If you have 2 GPUs but only want to use 1 for passthrough*, only do the 'set-override' below for the one you want to passthrough. 
    + Modern GPUs have at least 2 device IDs. 
        - First (0b:00:0) is the video output
        - Second (0b:00.1) is the audio output
        - Newer AMD GPUs may have more
        - Very old GPUs may only have video
* *No easy copy-paste block for these commands to remind you to update for* ***your IOMMU Ids*** before using.*


* `apt install driverctl`
* `driverctl list-devices` ... shows which devices are on what modules currently. Nvidia GPUs will look like this (AMD but is similar with different module names, same for future Intel GPUs):
> 0000:0c:00.0 nouveau  
> 0000:0c:00.1 snd_hda_intel  
   + **Make note of the defaults** to re-attach video to console later (these work as shown on Nvidia but if using AMD or Intel GPUs, need to know what modules booted by default).
* `driverctl list-overrides` ... empty for now (ignore error)

---

### 05.D.i. Override GPU modules

**NOTES:**

* This is where console video will disconnect if overriding all GPUs. 
* *This persists through reboot.*
* **Setting both GPUs (0b and 0c) here**, if 1 then you only need 2 commands (unless your GPU has more devices). 
* Edit these to match your IDs:  
`driverctl set-override 0000:0b:00.0 vfio-pci`  
`driverctl set-override 0000:0b:00.1 vfio-pci`  
`driverctl set-override 0000:0c:00.0 vfio-pci`  
`driverctl set-override 0000:0c:00.1 vfio-pci`  

* `driverctl list-devices` for GPU now looks like this for overridden GPUs ("[\*]" means it is set via override and persists):  
> 0000:0c:00.0 vfio-pci [\*]  
> 0000:0c:00.1 vfio-pci [\*]  

* `driverctl list-overrides` ... all current overrides (no "[\*]" shown as is implied by 'list-overrides'). 

### 05.D.ii. Reverting 'driverctl' Overrides

This will set your system back to default for whichever devices unset here. Reboot will be needed. 

**NOTE:** Don't do this right now, these are for referrence if you want to change it in the future

* *To unset the overrides* (returns a device to be used, after reboot, the way the system was before 'driverctl' use)  
`driverctl unset-override 0000:0c.0`  
`driverctl unset-override 0000:0c.1`  
    + *reboot* to get back to this configuration, but:
        - *Console video will not reappear you 'unset-override' until reboot.* 
        - If you want console video **now**, see next:

### 05.D.iii. Switch between Console Video and VFIO

This is the most useful for single GPU configurations, allowing you to use console output when needed. 

**NOTES:**

* I'm re-attaching the small GPU. 
* This section is why you made note of the module names from `driverctl list-devices` in the initial 'driverctl' setup. 
    + **If you don't remember what those were:**
        - do "Reverting `driverctl Overrides" from above and reboot, then:
        - `driverctl list-devices` after the reboot
* After running these, your console video should appear immediately on *without rebooting* (assuming this was the device used for console video at boot). 
* *Any 'set-override' persists throuh reboot.*
* **IMPORTANT:** Don't change overrides while a VM or container is actively using the GPU. It won't break your hardware or VM but this will happen:
    + The VM will lose the GPU display until restarted (RDP and/or VNC should still work)
    + The host will lose the ability to bind to the GPU with 'nouveau' to see console video
    + Just remember to shut down the VM prior to setting a different override. We're safe right now as we have no running VMs. 

1. Override to the modules used on a default boot (ie, turn on console video output):  
`driverctl set-override 0000:0c.0 nouveau`  
`driverctl set-override 0000:0c.1 snd_hda_intel`  
2. Override back to 'vfio-pci' for passing through to a VM (turns console video off):  
`driverctl set-override 0000:0c.0 vfio-pci`  
`driverctl set-override 0000:0c.1 vfio-pci`  

---

At this point we are able to create a Windows 10 virtual machine with a passed through GPU {[Next Page](06.VMWindows10.md)}.

---
> [^ [TOP OF PAGE](#proxmox-ve-71-nas-and-gaming-vms---05---proxmox-gpu-passthrough)] ... ***End:*** *Proxmox VE 7.1 NAS and Gaming VMs - Proxmox GPU Passthrough*
> 
> \> NEXT: [06 - VM Windows 10](06.VMWindows10.md)
>
> \< PREV: [04 - Proxmox Extras](04.ProxmoxExtras.md)
