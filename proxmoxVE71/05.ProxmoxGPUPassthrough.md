# Proxmox VE 7.1 NAS and Gaming VMs - 05 - Proxmox GPU Passthrough

***This file is under heavy revision and not currently usable***

This is how I configure my system for GPU Passthrough. It is a bit different than some other guides (no blacklisting, 'driverctl' instead). 

## Table of Contents:
> **Hint:** Github has a drop-down automatic TOC, find the **â‰¡** icon on the top-left

* ^ [README](README.md)  (*links to* ***other pages***)
* 05 - Proxmox GPU Passthrough (***this page***)
    + [05.A. VFIO Kernel Modules](#05a-vfio-kernel-modules)
    + [05.B. Boot Parameters](#05b-boot-parameters)
    + [05.C. GPU IDs](#05c-gpu-ids)
    + [05.D. driverctl Hookscript](#05d-driverctl-hookscript)
        - [05.D.i. Override GPU Modules](#05di-override-gpu-modules)
        - [05.D.ii. Reverting 'drivertctl' Overrides](#05dii-reverting-driverctl-overrides)
        - [05.D.iii. Switch Between Console Video and VFIO](#05diii-switch-between-console-video-and-vfio)
* \> [06 - VM Windows 10](06.VMWindows10.md) (*next page*)

**NOTES:**

* In BIOS settings on [01 - Hardware](01.Hardware.md) I made a note about setting my boot GPU device (console video) to PCIE Slot 2, this is where knowing this starts to matter.
* **Not meant to be a universal guide**, this page has specifics for my AMD Ryzen + Nvidia GPUs. But *can be adapted to other setups*.
* This will be useful for both single GPU, dual GPU with 1 used for the host, and dual passthrough GPU configs. 
* I used ZFS on the host boot disk (rpool). If using grub there is extra info but I can't test it. 

---

## 05.A. VFIO Kernel Modules

* **NOTE:** other Linux distributions may have these already built in to the kernel, but Proxmox still uses these as modules
    + Verify this via `cat /boot/config-5.13.19-6-pve |grep -i vfio` (update the name of 'config' for the kernel installed, it will change on updates)
    + Just something to be aware of as guides in the future will likely start omitting this step as it becomes unnecessary
    + Add these lines to '/etc/modules':
        - vfio
        - vfio_iommu_type1
        - vfio_pci
        - vfio_virqfd

```bash
# Copy/Paste this block into a root bash shell
FILE=/etc/modules
cat << EOF >> $FILE

vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd

EOF
echo; echo "Done. $FILE contents:"; echo
cat $FILE; echo
``` 

---

## 05.B. Boot Parameters

This is where you may have some trial and error. 

*Reminder:* My system has 2 GPUs in the first 2 PCIE slots. The first PCIE slot has my "big" GPU, the second PCIE slot has the smaller GPU. I want to keep the big GPU free for VM use. If you have a different configuration, *some* of the steps below will be different. 

**NOTE:** 'VT' below stands for 'Virtual Terminal'. This is what provides the text login screen and allows you to use a keyboard/mouse on the host without a UI. Aka "console" in a lot of other pages. 

* Add kernel boot parameters (if you have other options on your cmdline, add the following, don't replace the other items)
    + The place you will add these depends on whether you use systemd or grub to boot
    + The string to add: `quiet amd_iommu=on iommu=pt`
        - `quiet` ... leave this out *If you want to see detailed boot info*, using it speeds up boot at the expense of info
        - `amd_iommu=on` is **critical** to following steps working
        - `iommu=pt` may not be required
        - You can also add `fbcon=map:1` to the above if you have 2 GPUs and want VT monitor output on the second GPU (allows VT to be active while the big GPU is used by a VM, is temporarily disabled when using the small GPU with a VM).  *Leaving this out isn't a problem*, the VT will be active on the first GPU and go away when used by a VM (but returns after VM is shut down). 
          Example:  
	  > `quiet amd_iommu=on iommu=pt fbcon=map:1`
        - *if you have problems attaching to the GPUs* ***later*** you can add these to the above, but for my system it hasn't been needed. Try each to see. Ideally you won't need any of these, giving you the best chance to have VT active if needed.
        - `nomodeset` ... don't change display modes
        - `nofb` ... disable framebuffer on the VT
        - `video=efifb:off` ... disable framebuffer for EFI
        - `video=vesafb:off,efifb:off` ... disable VESA modes AND framebuffer for EFI
        - Example including ALL of the above (you can add all to see if your system boots and attaches GPUs properly, then test removing some):
            > `quiet amd_iommu=on iommu=pt fbcon=map:1 nomodeset nofb video=vesafb:off,efifb:off`
    + systemd
        - Add to '/etc/kernel/cmdline':  
        - Example with minimum added:
	    > root=ZFS=rpool/ROOT/pve-1 boot=zfs quiet amd_iommu=on iommu=pt
        - Example with everything added:  
	    > root=ZFS=rpool/ROOT/pve-1 boot=zfs quiet amd_iommu=on iommu=pt fbcon=map:1 nomodeset nofb video=vesafb:off,efifb:off
    + grub 
        - Add to the CMDLINE in '/etc/default/grub'
        - Example with minimum added:  
	    > GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on iommu=pt fbcon=map:1"
        - Example with everything added:  
	    > GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on iommu=pt fbcon=map:1 nomodeset nofb video=vesafb:off,efifb:off"
* Apply the above changes:
    + `proxmox-boot-tool refresh`
    + **Reboot now** to apply the change

---

## 05.C. GPU IDs

Use `iommu_list` (created on [this page](04.ProxmoxExtras.md#04b-useful-utilities)) to see the GPU device IDs:

*Example of the output for my system config:*

<pre>
{snip}
IOMMU Group 28:
	0b:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:2208] (rev a1)
	0b:00.1 Audio device [0403]: NVIDIA Corporation GA102 High Definition Audio Controller [10de:1aef] (rev a1)
IOMMU Group 29:
	0c:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 6GB] [10de:1c03] (rev a1)
	0c:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1)
{snip}
</pre>

* Modern GPUs have at least 2 device IDs
    + Your GPU may have more devices (USB, etc)
    + Very old GPUs may only have video
* My system has 2 different GPU models, makes it easy to know which is on each PCI id. 
    + Big GPU = RTX 3080TI (first PCIE slot)
        - First (0b:00:0) is the video output
        - Second (0b:00.1) is the audio output
    + Little GPU  = GTX 1060 6GB (second PCIE slot)
        - First (0c:00:0) is the video output
        - Second (0c:00.1) is the audio output
* Your device IDs will likely be different, ie '02:00.0' instead of '0b:00.0', etc.
* *With the 'driverctl' below* add '0000:' to those IDs.    
     **Example:** '0b:00.0' becomes '0000:0b:00.0'.

---

## 05.D. driverctl Hookscript

This section is for passing through PCI devices using a 'hookscript'. 

Doing this allows all GPUs to be used for VMs but return the Virtual Terminal video output when that GPU is not claimed by a VM. 

This is not needed *If you are are* ***dedicating a separate GPU for console output*** *and won't want to switch VMs often.* If you are doing single GPU passthrough (or using all GPUs for passthrough) then this is very useful. 

Feel free to test your VM out (see [06 - VM Windows 10](06.VMWindows10.md) for creating one) before coming back here if you want to add this hookscript. If you're happy with running the VM without extra work. I go ahead and do this now. 

### 05.D.i. driverctl Install

 'driverctl' is a part of the Debian Bullseye repositories. I use it to ***override*** a GPU temporarily *instead of blacklisting modules*. It allows reassigning devices repeatedly without rebooting **while using the Linux virtual terminal** when a VM isn't active on that GPU ([originally found here](https://www.heiko-sieger.info/blacklisting-graphics-driver/)).
 
**NOTES:** 
* *If you override the GPU used for the virtual terminal,* ***video output for the virtual terminal will stop while the VM is using the GPU.***

* `apt install driverctl`
* `driverctl list-devices` ... shows which devices are on what modules currently. Nvidia GPUs will look like this (AMD but is similar with different module names, same for future Intel GPUs):  
    > 0000:0c:00.0 nouveau  
    > 0000:0c:00.1 snd_hda_intel  
* `driverctl list-overrides` ... shows any overrides set (empty for now, ignore error)

### 05.D.ii. Create Hookscript

We can use a "hookscript" that runs automatically whenever the VM is started or stopped instead of using 'driverctl' in the shell each time. 

**More Information:**

* [Proxmox docs](https://pve.proxmox.com/pve-docs/pve-admin-guide.html#_hookscripts)
* [Proxmox - Execute Script on Host](https://blog.infoitech.co.uk/proxmox-execute-script-on-host/)
* [Proxmox PCI Switcher](https://github.com/rosineygp/proxmox-pci-switcher) ... a Python *alternative* to my hookscript. 
    + If I understand it correctly, PCI Switcher will automatically shutdown a running VM using a PCI to start a new VM that needs the same PCI device
    + My script below will fail to start a VM if a PCI device needed is in use
    + Up to you to figure out which you want
        - If you prefer the PCI Switcher method much of the information below will be the same but you'll need to adapt the directions.
        - Skip the information below related to 'driverctl' if you go with PCI Switcher

**Create the hookscript:**

* Edit '/etc/pve/storage.cfg'
    + find "        content iso,vztmpl,backup"
    + add ",snippets"  
        Result: "        content iso,vztmpl,backup,snippets"
    + save the file
    + Shift+Reload the Proxmox web UI
        - Proxmox UI will add a new item: 'Datacenter' > '[your host]' > 'local (your host)' (storage) > 'Snippets' (middle column)
        - Proxmox will create a file directory '/var/lib/vz/snippets'
        - *note:* You could make the directory in shell, but you also want Proxmox to know about the directory
* Open [hookscript-driverctl.pl](hookscript-driverctl.pl) in a new browser window
     + Copy the contents of that script (it's long, so not embedded here)
     + Paste the contents into **'/var/lib/vz/snippets/hookscript-driverctl.pl'
     + `chmod +x /var/lib/vz/hookscript-driverctl.pl`

### 05.D.iii. Attach Hookscript

**NOTE:** This won't be available until you have created a VM (example on [06.VM Windows 10](06.VMWindows10.md)). If you haven't done that yet, just come back here when ready. 

* Determine the <vmid> for the passthrough VM. The first VM you created will be '100' unless you changed that manually
*  In a shell do this: `qm set <vmid> -hookscript local:snippets/driverctl.pl`  
    Example: `qm set 100 -hookscript local:snippets/driverctl.pl`
* At this point this script will be active:
    + 'pre-start' ... sets overrides on PCI devices before starting the VM (but after you press the UI 'Start' button)
    + 'post-stop' ... unsets overrides after the VM has been shut down via the UI 'Shutdown' button

**NOTES:**

* Hookscripts can be in different languages
    + The example given in the Proxmox docs use Perl and I was already comfortable with that
    + Python is used by the PCI Switcher script linked above
    + Bash and other languages also work
* Hookscripts can also be activated during the 'post-start' phase (runs once the VM is running) and 'pre-stop' phase (runs just before shutting down the VM). However this hookscript doesn't need to do anything there
* VMs can only have a single hookscript. If you want to do additional items:
    + Copy 'hookscript-driverctl.pl' to a new file like 'my-hookscript.pl' (whatever name you want)
     + Add your custom commands to the new file
     + Attach the new hookscript via `qm set <vmid> -hookscript local:snippets/my-hookscript.pl`
    + Multiple VMs can use the same hookscript, attach to the new <vmid> 
    + Remove a hookscript from a VM via `qm set <vmid> --delete hookscript`
* *Promox UI and Hookscripts:*
    + You can *see* which hookscript the VM has in the 'Options' for that VM in the Proxmox UI, but you can't change it from the UI (see last few lines above for how to modify the hookscript or remove it from the VM)
    + In the Proxmox UI 'Tasks' / 'Task History'
        - See what happened with the hookscript *in the 'pre-start' phase*  (Task: "VM <vmid> - Start")
        - Can't see what happens ***in the 'post-stop'*** phase 
* *Syslog:*
    + For 'hookscript-driverctl.pl' I've added code to send output to syslog as well so that you can see 'post-start' information.
        - Proxmox UI ... use the 'Syslog' of the host in UI
        - Via shell:  
            `grep hookscript-driverctl.pl /var/log/syslog`
    + If debugging gets hard, you can start/stop the VM from shell (see [05.E.i](#05ei-stop-and-start-vm-from-shell) below) to see 'qmeventd' events in syslog. 

* Attach the hookscript to VMs that should use it

---

## 05.E. Extra Information

You don't need to do anything on your host with the rest of this page. These are just pointers on how to do some related things. 

### 05.E.i. Stop and Start VM from shell

If you want to start/stop VMs in the shell:

* `qm start <vmid>` is basically the same as pressing 'Start' in the UI
* `qm stop <vmid>` is basically the same as pressing 'Shutdown' in the UI
    + if a VM is unable to shutdown, do `qm stop <vmid> -skiplock` ... this can shutdown VMs that won't shutdown via the UI. *Use with caution.*
* Doing the above will send output to syslog that would normally be captured by the UI 'Tasks' view
    + This can be useful for seeing output in syslog for 'qmeventd' that the UI doesn't capture (like in the 'post-stop' phase). 
    + I've made my hookscript send extra info to syslog, this would be more needed by other hookscripts that might not do that or to see info without using the UI while debugging. 

### 05.E.ii. How to  use driverctl manually

You don't need to manually use `driverctl` once the hookscript is running, but this section will give you information on how it works. **No need to run these commands now.**

**Bind GPU devices to the VFIO module:**

* '--nosave' tells driverctl to override now but not permanently. This allows something to go wrong and not prevent reboot from working. 
* 'set-override' has driverctl bind the device (ie, '0000:0b:00.0') to the 'vfio-pci' module
* **Edit these to match your IDs:**  
    `driverctl --nosave set-override 0000:0b:00.0 vfio-pci`  
    `driverctl --nosave set-override 0000:0b:00.1 vfio-pci`  
    `driverctl --nosave set-override 0000:0c:00.0 vfio-pci`  
    `driverctl --nosave set-override 0000:0c:00.1 vfio-pci`  
* *virtual terminal video goes blank at this point if one of those GPUs is used for it*
* If you have a passthrough VM already created, you can start it now to see if passthrough worked. 

**See device bindings:**

* `driverctl list-devices` for GPU(s) now looks like this for overridden GPUs  
    ("[\*]" means it is set via override and persists):  
> 0000:0b:00.0 vfio-pci [\*]  
> 0000:0b:00.1 vfio-pci [\*]  
> 0000:0c:00.0 vfio-pci [\*]  
> 0000:0c:00.1 vfio-pci [\*]  

**See current overrides:**

* `driverctl list-overrides` ... all current overrides  
    (no "[\*]" shown, implied by 'list-overrides'). 

**Remove overrides:**

* If you are testing the prior commands in a VM, **shut down the VM before doing this.**
* Video will bind to 'nouveau' for Nvidia GPUs, audio will either rebind to 'snd_hda_intel' or remain on 'vfio-pci' if the system wasn't using it. 
    `driverctl --nosave unset-override 0000:0b.0`  
    `driverctl --nosave unset-override 0000:0b.1`  
    `driverctl --nosave unset-override 0000:0c.0`  
    `driverctl --nosave unset-override 0000:0c.1`  
* *virtual terminal video will reappear at this point, press ENTER to see the login prompt*

**--noprobe:**

If you add the **'--noprobe'** argument (like '--nosave' shown above) the command will try to execute regardless of whether the device is actively in use. This can help if you've done something that messed up a binding. But you may still end up in a "better to just reboot" situation. That's actually the reason to use '--nosave' argument, so you can reboot without having a change that might break things.  

---

## 05.E. Extra pointers


# fix terminal video 
# Not needed on all monitors, but my U3011 **using DVID-D** does require this
# comment out to test
echo 0 > /sys/class/vtconsole/vtcon1/bind
echo 1 > /sys/class/vtconsole/vtcon1/bind
setupcon

fonts in /usr/share/consolefonts

qm stop 100 -skiplock

**Incorporate:** https://www.nicksherlock.com/2020/11/working-around-the-amd-gpu-reset-bug-on-proxmox/


---






At this point we are able to create a Windows 10 virtual machine with a passed through GPU {[Next Page](06.VMWindows10.md)}.

---
> [^ [TOP OF PAGE](#proxmox-ve-71-nas-and-gaming-vms---05---proxmox-gpu-passthrough)] ... ***End:*** *Proxmox VE 7.1 NAS and Gaming VMs - Proxmox GPU Passthrough*
> 
> \> NEXT: [06 - VM Windows 10](06.VMWindows10.md)
>
> \< PREV: [04 - Proxmox Extras](04.ProxmoxExtras.md)
