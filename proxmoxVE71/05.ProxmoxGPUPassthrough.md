# Proxmox VE 7.1 NAS and Gaming VMs - 05 - Proxmox GPU Passthrough

***This file is under heavy revision and not currently usable***

This is how I configure my system for GPU Passthrough. It is a bit different than some other guides (no blacklisting, 'driverctl' instead). 

## Table of Contents:
> **Hint:** Github has a drop-down automatic TOC, find the **â‰¡** icon on the top-left

* ^ [README](README.md)  (*links to* ***other pages***)
* 05 - Proxmox GPU Passthrough (***this page***)
    + [05.A. VFIO Kernel Modules](#05a-vfio-kernel-modules)
    + [05.B. Boot Parameters](#05b-boot-parameters)
    + [05.C. GPU IDs](#05c-gpu-ids)
    + [05.D. driverctl Hookscript](#05d-driverctl-hookscript)
        - [05.D.i. Override GPU Modules](#05di-override-gpu-modules)
        - [05.D.ii. Reverting 'drivertctl' Overrides](#05dii-reverting-driverctl-overrides)
        - [05.D.iii. Switch Between Console Video and VFIO](#05diii-switch-between-console-video-and-vfio)
* \> [06 - VM Windows 10](06.VMWindows10.md) (*next page*)

**NOTES:**

* In BIOS settings on [01 - Hardware](01.Hardware.md) I made a note about setting my boot GPU device (console video) to PCIE Slot 2, this is where knowing this starts to matter.
* **Not meant to be a universal guide**, this page has specifics for my AMD Ryzen + Nvidia GPUs. But *can be adapted to other setups*.
* This will be useful for both single GPU, dual GPU with 1 used for the host, and dual passthrough GPU configs. 
* I used ZFS on the host boot disk (rpool). If using grub there is extra info but I can't test it. 

---

## 05.A. VFIO Kernel Modules

* **NOTE:** other Linux distributions may have these already built in to the kernel, but Proxmox still uses these as modules
    + Verify this via `cat /boot/config-5.13.19-6-pve |grep -i vfio` (update the name of 'config' for the kernel installed, it will change on updates)
    + Just something to be aware of as guides in the future will likely start omitting this step as it becomes unnecessary
    + Add these lines to '/etc/modules':
        - vfio
        - vfio_iommu_type1
        - vfio_pci
        - vfio_virqfd

```bash
# Copy/Paste this block into a root bash shell
FILE=/etc/modules
cat << EOF >> $FILE

vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd

EOF
echo; echo "Done. $FILE contents:"; echo
cat $FILE; echo
``` 

---

## 05.B. Boot Parameters

This is where you may have some trial and error. 

*Reminder:* My system has 2 GPUs in the first 2 PCIE slots. The first PCIE slot has my "big" GPU, the second PCIE slot has the smaller GPU. I want to keep the big GPU free for VM use. If you have a different configuration, *some* of the steps below will be different. 

**NOTE:** 'VT' below stands for 'Virtual Terminal'. This is what provides the text login screen and allows you to use a keyboard/mouse on the host without a UI. Aka "console" in a lot of other pages. 

* Add kernel boot parameters (if you have other options on your cmdline, add the following, don't replace the other items)
    + The place you will add these depends on whether you use systemd or grub to boot
    + The string to add: `quiet amd_iommu=on iommu=pt`
        - `quiet` ... leave this out *If you want to see detailed boot info*, using it speeds up boot at the expense of info
        - `amd_iommu=on` is **critical** to following steps working
        - `iommu=pt` may not be required
        - You can also add `fbcon=map:1` to the above if you have 2 GPUs and want VT monitor output on the second GPU (allows VT to be active while the big GPU is used by a VM, is temporarily disabled when using the small GPU with a VM).  *Leaving this out isn't a problem*, the VT will be active on the first GPU and go away when used by a VM (but returns after VM is shut down). 
          Example:  
	  > `quiet amd_iommu=on iommu=pt fbcon=map:1`
        - *if you have problems attaching to the GPUs* ***later*** you can add these to the above, but for my system it hasn't been needed. Try each to see. Ideally you won't need any of these, giving you the best chance to have VT active if needed.
        - `nomodeset` ... don't change display modes
        - `nofb` ... disable framebuffer on the VT
        - `video=efifb:off` ... disable framebuffer for EFI
        - `video=vesafb:off,efifb:off` ... disable VESA modes AND framebuffer for EFI
        - Example including ALL of the above (you can add all to see if your system boots and attaches GPUs properly, then test removing some):
            > `quiet amd_iommu=on iommu=pt fbcon=map:1 nomodeset nofb video=vesafb:off,efifb:off`
    + systemd
        - Add to '/etc/kernel/cmdline':  
        - Example with minimum added:
	    > root=ZFS=rpool/ROOT/pve-1 boot=zfs quiet amd_iommu=on iommu=pt
        - Example with everything added:  
	    > root=ZFS=rpool/ROOT/pve-1 boot=zfs quiet amd_iommu=on iommu=pt fbcon=map:1 nomodeset nofb video=vesafb:off,efifb:off
    + grub 
        - Add to the CMDLINE in '/etc/default/grub'
        - Example with minimum added:  
	    > GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on iommu=pt fbcon=map:1"
        - Example with everything added:  
	    > GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on iommu=pt fbcon=map:1 nomodeset nofb video=vesafb:off,efifb:off"
* Apply the above changes:
    + `proxmox-boot-tool refresh`
    + **Reboot now** to apply the change

---

## 05.C. GPU IDs

Use `iommu_list` (created on [this page](04.ProxmoxExtras.md#04b-useful-utilities)) to see the GPU device IDs:

*Example of the output for my system config:*

<pre>
{snip}
IOMMU Group 28:
	0b:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:2208] (rev a1)
	0b:00.1 Audio device [0403]: NVIDIA Corporation GA102 High Definition Audio Controller [10de:1aef] (rev a1)
IOMMU Group 29:
	0c:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP106 [GeForce GTX 1060 6GB] [10de:1c03] (rev a1)
	0c:00.1 Audio device [0403]: NVIDIA Corporation GP106 High Definition Audio Controller [10de:10f1] (rev a1)
{snip}
</pre>

* Modern GPUs have at least 2 device IDs
    + Your GPU may have more devices (USB, etc)
    + Very old GPUs may only have video
* My system has 2 different GPU models, makes it easy to know which is on each PCI id. 
    + Big GPU = RTX 3080TI (first PCIE slot)
        - First (0b:00:0) is the video output
        - Second (0b:00.1) is the audio output
    + Little GPU  = GTX 1060 6GB (second PCIE slot)
        - First (0c:00:0) is the video output
        - Second (0c:00.1) is the audio output
* Your device IDs will likely be different, ie '02:00.0' instead of '0b:00.0', etc.
* *With the 'driverctl' below* add '0000:' to those IDs.    
     **Example:** '0b:00.0' becomes '0000:0b:00.0'.

---

## 05.D. driverctl Hookscript

This section is for passing through PCI devices using a 'hookscript'. 

Doing this allows all GPUs to be used for VMs but return the Virtual Terminal video output when that GPU is not claimed by a VM. 

This is not needed *If you are are* ***dedicating a separate GPU for console output*** *and won't want to switch VMs often.* If you are doing single GPU passthrough (or using all GPUs for passthrough) then this is very useful. 

Feel free to test your VM out (see [06 - VM Windows 10](06.VMWindows10.md) for creating one) before coming back here if you want to add this hookscript. If you're happy with running the VM without extra work. I go ahead and do this now. 

### 05.D.i. driverctl Install

 'driverctl' is a part of the Debian Bullseye repositories. I use it to ***override*** a GPU temporarily *instead of blacklisting modules*. It allows reassigning devices repeatedly without rebooting **while using the Linux virtual terminal** when a VM isn't active on that GPU ([originally found here](https://www.heiko-sieger.info/blacklisting-graphics-driver/)).
 
**NOTES:** 
* *If you override the GPU used for the virtual terminal,* ***video output for the virtual terminal will stop while the VM is using the GPU.***

* `apt install driverctl`
* `driverctl list-devices` ... shows which devices are on what modules currently. Nvidia GPUs will look like this (AMD but is similar with different module names, same for future Intel GPUs):  
    > 0000:0c:00.0 nouveau  
    > 0000:0c:00.1 snd_hda_intel  
* `driverctl list-overrides` ... shows any overrides set (empty for now, ignore error)

### 05.D.ii. How to use driverctl

You won't need to manually use `driverctl` once the hookscript is running, but this section will give you information on how it works. **Don't feel you need to run these commands now.**

**Bind GPU devices to the VFIO module:**

* '--nosave' tells driverctl to override now but not permanently. This allows something to go wrong and not prevent reboot from working. 
* 'set-override' has driverctl bind the device (ie, '0000:0b:00.0') to the 'vfio-pci' module
* **Edit these to match your IDs:**  
    `driverctl --nosave set-override 0000:0b:00.0 vfio-pci`  
    `driverctl --nosave set-override 0000:0b:00.1 vfio-pci`  
    `driverctl --nosave set-override 0000:0c:00.0 vfio-pci`  
    `driverctl --nosave set-override 0000:0c:00.1 vfio-pci`  
* *virtual terminal video goes blank at this point if one of those GPUs is used for it*
* If you have a passthrough VM already created, you can start it now to see if passthrough worked. 

**See device bindings:**

* `driverctl list-devices` for GPU(s) now looks like this for overridden GPUs  
    ("[\*]" means it is set via override and persists):  
> 0000:0b:00.0 vfio-pci [\*]  
> 0000:0b:00.1 vfio-pci [\*]  
> 0000:0c:00.0 vfio-pci [\*]  
> 0000:0c:00.1 vfio-pci [\*]  

**See current overrides:**

* `driverctl list-overrides` ... all current overrides  
    (no "[\*]" shown, implied by 'list-overrides'). 

**Remove overrides:**

* If you are testing the prior commands in a VM, **shut down the VM before doing this.**
* Video will bind to 'nouveau' for Nvidia GPUs, audio will either rebind to 'snd_hda_intel' or remain on 'vfio-pci' if the system wasn't using it. 
    `driverctl --nosave unset-override 0000:0b.0`  
    `driverctl --nosave unset-override 0000:0b.1`  
    `driverctl --nosave unset-override 0000:0c.0`  
    `driverctl --nosave unset-override 0000:0c.1`  
* *virtual terminal video will reappear at this point, press ENTER to see the login prompt*


### 05.D.iii. Hookscript

We can use a "hookscript" that runs automatically whenever the VM is started or stopped instead of using 'driverctl' in the shell each time. 

**More Information:**

* [Proxmox docs](https://pve.proxmox.com/pve-docs/pve-admin-guide.html#_hookscripts)
* https://blog.infoitech.co.uk/proxmox-execute-script-on-host/
* https://forum.proxmox.com/threads/how-to-use-new-hookscript-feature.53388/

* edit /etc/pve/storage.cfg
* change "        content iso,vztmpl,backup" to "        content iso,vztmpl,backup,snippets"

/var/lib/vz/snippets



# fix terminal video 
# Not needed on all monitors, but my U3011 **using DVID-D** does require this
# comment out to test
echo 0 > /sys/class/vtconsole/vtcon1/bind
echo 1 > /sys/class/vtconsole/vtcon1/bind
setupcon

fonts in /usr/share/consolefonts

qm stop 100 -skiplock

**Incorporate:** https://www.nicksherlock.com/2020/11/working-around-the-amd-gpu-reset-bug-on-proxmox/


---






At this point we are able to create a Windows 10 virtual machine with a passed through GPU {[Next Page](06.VMWindows10.md)}.

---
> [^ [TOP OF PAGE](#proxmox-ve-71-nas-and-gaming-vms---05---proxmox-gpu-passthrough)] ... ***End:*** *Proxmox VE 7.1 NAS and Gaming VMs - Proxmox GPU Passthrough*
> 
> \> NEXT: [06 - VM Windows 10](06.VMWindows10.md)
>
> \< PREV: [04 - Proxmox Extras](04.ProxmoxExtras.md)
